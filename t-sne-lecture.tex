% t-SNE Lecture - Slides 1-9 (Revised Slide 8 for aesthetics)
\documentclass{beamer}
\usetheme{Madrid}
\usecolortheme{seahorse}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows.meta, positioning}

% Define custom colors for consistency
\definecolor{upcblue}{RGB}{0,123,199}
\definecolor{upcgray}{RGB}{100,100,100}
\definecolor{highlight}{RGB}{255,127,0}

% Adjust beamer margins to prevent truncation
\setbeamersize{text margin left=5mm,text margin right=5mm}

\begin{document}

% SLIDE 1: Title Slide
\begin{frame}[plain]
\vspace{0.5cm}
\begin{center}
{\LARGE \textcolor{upcblue}{\textbf{Nonlinear Dimensionality Reduction:}}}\\[0.2cm]
{\huge \textcolor{upcblue}{\textbf{t-Stochastic Neighbor Embedding}}}\\[0.2cm]
{\huge \textcolor{upcblue}{\textbf{(t-SNE)}}}\\[1cm]

{\large \textbf{Prof. Endri Raco}}\\[0.2cm]
{\normalsize Polytechnic University of Tirana}\\[0.1cm]
{\small \textit{Erasmus+ Exchange Professor}}\\[0.8cm]

{\large \textcolor{upcgray}{Advanced Multivariate Analysis}}\\[0.2cm]
{\normalsize Polytechnic University of Catalonia (UPC)}\\[0.3cm]
{\normalsize October 15, 2025}
\end{center}

\vspace{0.3cm}
\begin{center}
\begin{tikzpicture}[scale=0.25]
% 3D point cloud on the left
\foreach \x in {0,1,2}
    \foreach \y in {0,1,2}
        \foreach \z in {0,1,2}
            \node[circle, fill=upcblue!20, inner sep=1pt] at (\x+\z*0.3,\y+\z*0.3) {};
            
% Arrow indicating transformation
\draw[->, thick, upcgray] (4,1.5) -- (6,1.5);

% 2D circular arrangement on the right
\foreach \angle in {0,45,...,315}
    \node[circle, fill=highlight!60, inner sep=2pt] at ({8+0.8*cos(\angle)},{1.5+0.8*sin(\angle)}) {};
\end{tikzpicture}
\end{center}
\end{frame}

% SLIDE 2: Welcome and Overview
\begin{frame}{Welcome to Advanced Multivariate Analysis}
\vspace{-0.2cm}
{\color{upcblue}\textbf{Today's Journey}}\\[0.2cm]
\begin{itemize}
    \setlength\itemsep{0.3em}
    \item \textbf{2-hour deep dive} into t-SNE
    \item \textbf{Mathematical foundations} to practical insights
    \item \textbf{Three key parts:}
    \begin{enumerate}
        \item SNE - The original idea
        \item t-SNE - Solving the crowding problem  
        \item Hyperparameters \& interpretation
    \end{enumerate}
\end{itemize}

\vspace{0.3cm}
\begin{center}
\begin{tikzpicture}[scale=0.8]
    \node[draw=upcblue, thick, rounded corners, 
          minimum width=5cm, minimum height=1.2cm,
          fill=upcblue!5] at (0,0) {
        \begin{minipage}{4.8cm}
        \centering
        $p_{j|i} = \frac{e^{-\|\mathbf{x}_i - \mathbf{x}_j\|^2/2\sigma_i^2}}{\sum_{k \neq i} e^{-\|\mathbf{x}_i - \mathbf{x}_k\|^2/2\sigma_i^2}}$
        \end{minipage}
    };
    \node at (0,-1) {\small \textcolor{upcgray}{Core t-SNE Formula (We'll derive this today)}};
\end{tikzpicture}
\end{center}
\end{frame}

% SLIDE 3: The Challenge of High-Dimensional Data
\begin{frame}{The Curse of Dimensionality}
\vspace{-0.3cm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{\color{upcblue}Our Intuition Works Here:}

\begin{center}
\begin{tikzpicture}[scale=0.65]
    % 2D scatter plot
    \node at (0,-1.3) {\footnotesize \textbf{2D: Simple}};
    \draw[->] (-1.2,0) -- (1.2,0) node[right] {\tiny $x_1$};
    \draw[->] (0,-0.8) -- (0,0.8) node[above] {\tiny $x_2$};
    
    % Cluster 1
    \foreach \i in {1,...,5} {
        \node[circle, fill=upcblue!60, inner sep=1pt] at ({-0.5+0.2*rand},{0.3+0.2*rand}) {};
    }
    % Cluster 2
    \foreach \i in {1,...,5} {
        \node[circle, fill=highlight!60, inner sep=1pt] at ({0.5+0.2*rand},{-0.3+0.2*rand}) {};
    }
\end{tikzpicture}
\end{center}

\begin{center}
\begin{tikzpicture}[scale=0.6]
    % 3D scatter plot
    \node at (0,-1.5) {\footnotesize \textbf{3D: Manageable}};
    \draw[->] (-1,0) -- (1.2,0) node[right] {\tiny $x_1$};
    \draw[->] (0,-0.8) -- (0,0.8) node[above] {\tiny $x_2$};
    \draw[->] (-0.5,-0.5) -- (0.5,0.5) node[above right] {\tiny $x_3$};
    
    % 3D clusters
    \foreach \i in {1,...,4} {
        \node[circle, fill=upcblue!60, inner sep=1pt] at ({-0.3+0.15*rand},{0.2+0.15*rand}) {};
    }
    \foreach \i in {1,...,4} {
        \node[circle, fill=highlight!60, inner sep=1pt] at ({0.3+0.15*rand},{-0.2+0.15*rand}) {};
    }
\end{tikzpicture}
\end{center}
\end{column}

\begin{column}{0.48\textwidth}
\textbf{\color{upcblue}But Not Here:}

\begin{center}
\begin{tikzpicture}
    \draw[upcgray, thick, rounded corners, fill=upcgray!10] 
          (-1.2,-0.8) rectangle (1.2,0.8);
    \node at (0,0) {\Large \textbf{?}};
    \node at (0,-1.3) {\footnotesize \textbf{100D? 1000D?}};
\end{tikzpicture}
\end{center}

\vspace{0.3cm}
\textcolor{upcgray}{\textbf{Problems:}}
\begin{itemize}
    \small
    \item \textcolor{highlight}{Distance concentration}
    \item \textcolor{highlight}{Volume:} $V_n(r) \propto r^n$
    \item \textcolor{highlight}{Sparse data}
\end{itemize}

\vspace{0.2cm}
\centering
\small\textit{\color{upcblue}``Geometric intuition fails''}
\end{column}
\end{columns}
\end{frame}

% SLIDE 4: Why Reduce Dimensions?
\begin{frame}{Goals of Dimensionality Reduction}
\vspace{-0.3cm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{\color{upcblue}Goal 1: Visualization}

\begin{center}
\begin{tikzpicture}[scale=0.7]
    % High-dimensional
    \node[draw=upcgray, minimum width=2cm, minimum height=1.5cm, fill=upcgray!10] 
          at (0,0) {
        \begin{minipage}{1.8cm}
        \centering
        \footnotesize
        $\mathbf{x} \in \mathbb{R}^{784}$\\
        (MNIST)
        \end{minipage}
    };
    
    % Arrow
    \draw[->, thick, upcblue] (0,-1.2) -- (0,-2.2) 
          node[midway, right] {\footnotesize compress};
    
    % Low-dimensional
    \node[draw=upcblue, minimum width=2cm, minimum height=1.2cm, fill=upcblue!10] 
          at (0,-3) {
        \begin{minipage}{1.8cm}
        \centering
        \footnotesize
        $\mathbf{y} \in \mathbb{R}^{2}$\\
        (2D plot)
        \end{minipage}
    };
\end{tikzpicture}
\end{center}

\small
\textcolor{upcgray}{\textbf{Key:}} ``See'' hidden structure
\end{column}

\begin{column}{0.48\textwidth}
\textbf{\color{upcblue}Goal 2: Feature Extraction}

\begin{center}
\begin{tikzpicture}[scale=0.7]
    % Original
    \node[draw=upcgray, minimum width=2cm, minimum height=1.2cm, fill=upcgray!10] 
          at (0,0) {
        \begin{minipage}{1.8cm}
        \centering
        \footnotesize
        Noisy\\features
        \end{minipage}
    };
    
    % Arrow
    \draw[->, thick, highlight] (0,-1) -- (0,-2) 
          node[midway, right] {\footnotesize extract};
    
    % Essential
    \node[draw=highlight, minimum width=2cm, minimum height=1.2cm, fill=highlight!10] 
          at (0,-2.8) {
        \begin{minipage}{1.8cm}
        \centering
        \footnotesize
        Essential\\structure
        \end{minipage}
    };
\end{tikzpicture}
\end{center}

\small
\textcolor{upcgray}{\textbf{Benefits:}}
\begin{itemize}
    \footnotesize
    \item Noise reduction
    \item Efficiency
    \item Better ML
\end{itemize}
\end{column}
\end{columns}

\vspace{0.3cm}
\begin{center}
\colorbox{upcblue!10}{
\begin{minipage}{0.85\textwidth}
\centering
\small\textbf{Challenge:} Preserve relationships while reducing dimensions
\end{minipage}
}
\end{center}
\end{frame}

% SLIDE 5: The Limits of Linear Projections
\begin{frame}{When Linear Methods Falter}
\vspace{-0.3cm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{\color{upcblue}Swiss Roll Dataset}

\begin{center}
\begin{tikzpicture}[scale=0.6]
    % Swiss Roll curves
    \draw[thick, upcblue!40] plot[smooth, tension=0.5] 
          coordinates {(-1.2,0) (-0.8,0.4) (-0.2,0.6) (0.4,0.4) (0.8,0) (1.2,-0.4)};
    \draw[thick, upcblue!60] plot[smooth, tension=0.5] 
          coordinates {(-1,0.3) (-0.6,0.7) (0,0.9) (0.6,0.7) (1,0.3)};
    \draw[thick, upcblue!80] plot[smooth, tension=0.5] 
          coordinates {(-0.8,0.6) (-0.4,1) (0.2,1.2) (0.8,1)};
    
    % Points with colors
    \node[circle, fill=red!80, inner sep=1.5pt] at (-1,0.3) {};
    \node[circle, fill=orange!80, inner sep=1.5pt] at (-0.3,0.8) {};
    \node[circle, fill=yellow!80, inner sep=1.5pt] at (0.3,1) {};
    \node[circle, fill=green!80, inner sep=1.5pt] at (0.8,0.5) {};
    \node[circle, fill=blue!80, inner sep=1.5pt] at (1.1,-0.2) {};
    
    \node at (0,-1.3) {\footnotesize \textbf{3D Manifold}};
\end{tikzpicture}
\end{center}

\small
\textcolor{upcgray}{\textbf{True Structure:}}\\
\footnotesize 2D manifold in 3D space
\end{column}

\begin{column}{0.48\textwidth}
\textbf{\color{upcblue}PCA Projection}

\begin{center}
\begin{tikzpicture}[scale=0.6]
    % PCA axes
    \draw[->] (-1.5,0) -- (1.5,0) node[right] {\tiny PC1};
    \draw[->] (0,-1) -- (0,1) node[above] {\tiny PC2};
    
    % Mixed colors - PCA failure
    \node[circle, fill=red!80, inner sep=1.5pt] at (-0.2,0.3) {};
    \node[circle, fill=blue!80, inner sep=1.5pt] at (0.1,0.4) {};
    \node[circle, fill=orange!80, inner sep=1.5pt] at (0.3,0.2) {};
    \node[circle, fill=green!80, inner sep=1.5pt] at (-0.1,-0.2) {};
    \node[circle, fill=yellow!80, inner sep=1.5pt] at (0.2,-0.3) {};
    
    \node at (0,-1.3) {\footnotesize \textbf{PCA Result}};
\end{tikzpicture}
\end{center}

\small
\textcolor{highlight}{\textbf{Problem:}}\\
\footnotesize Preserves variance,\\
\footnotesize destroys local structure
\end{column}
\end{columns}

\vspace{0.2cm}
\begin{center}
\colorbox{highlight!20}{
\begin{minipage}{0.85\textwidth}
\centering
\small\textit{Need methods that preserve \textbf{local relationships}}
\end{minipage}
}
\end{center}
\end{frame}

% SLIDE 6: The Manifold Hypothesis
\begin{frame}{The Manifold Hypothesis}
\vspace{-0.3cm}
\begin{center}
\colorbox{upcblue!10}{
\begin{minipage}{0.9\textwidth}
\centering
\textit{``High-dimensional data often lies on or near}\\
\textit{a much lower-dimensional manifold''}
\end{minipage}
}
\end{center}

\vspace{0.3cm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{\color{upcblue}Example: Earth's Surface}

\begin{center}
\begin{tikzpicture}[scale=0.7]
    % Earth sphere
    \shade[ball color=blue!30] (0,0) circle (1cm);
    \draw[thick] (0,0) circle (1cm);
    
    % Continents
    \fill[green!60, opacity=0.7] (0.2,0.3) circle (0.2cm);
    \fill[green!60, opacity=0.7] (-0.3,-0.2) circle (0.15cm);
    
    % Arrow
    \draw[->, thick, highlight] (1.5,0.5) -- (0.8,0.3) 
          node[above, midway] {\tiny 2D};
    
    \node at (0,-1.5) {\footnotesize \textbf{2D surface in 3D}};
\end{tikzpicture}
\end{center}
\end{column}

\begin{column}{0.48\textwidth}
\textbf{\color{upcblue}Mathematical Form}

\small
Data: $\mathbf{X} = \{\mathbf{x}_1, ..., \mathbf{x}_n\}$\\
where $\mathbf{x}_i \in \mathbb{R}^D$

\vspace{0.2cm}
\textbf{Assumption:}\\
$\exists$ manifold $\mathcal{M}$ with dim $d \ll D$:

$$\mathbf{x}_i \approx f(\mathbf{z}_i) + \epsilon_i$$

\footnotesize
\begin{itemize}
    \item $\mathbf{z}_i \in \mathbb{R}^d$ (low-dim)
    \item $f: \mathbb{R}^d \rightarrow \mathbb{R}^D$
    \item $\epsilon_i$ (noise)
\end{itemize}
\end{column}
\end{columns}

\vspace{0.2cm}
\begin{center}
\colorbox{upcgray!10}{
\begin{minipage}{0.85\textwidth}
\centering
\small\textbf{Goal:} Uncover this hidden low-dimensional structure
\end{minipage}
}
\end{center}
\end{frame}

% SLIDE 7: A Glimpse of t-SNE
\begin{frame}{Preserving Neighborhoods: t-SNE in Action}
\vspace{-0.2cm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{\color{upcblue}PCA on MNIST Digits}

\begin{center}
\begin{tikzpicture}[scale=0.6]
    % PCA axes
    \draw[->] (-1.8,0) -- (1.8,0) node[right] {\tiny PC1};
    \draw[->] (0,-1.8) -- (0,1.8) node[above] {\tiny PC2};
    
    % Overlapping clusters - PCA
    \foreach \x/\y/\c in {-0.4/0.4/red!60, 0.3/0.5/blue!60, -0.2/-0.3/green!60, 
                          0.3/-0.2/orange!60, -0.3/0.1/purple!60} {
        \foreach \i in {1,...,6} {
            \pgfmathsetmacro{\rx}{0.35*rand}
            \pgfmathsetmacro{\ry}{0.35*rand}
            \node[circle, fill=\c, inner sep=1pt, opacity=0.6] at (\x+\rx,\y+\ry) {};
        }
    }
    
    \node at (0,-2.2) {\tiny \textbf{PCA: Mixed Clusters}};
\end{tikzpicture}
\end{center}

\footnotesize
\textcolor{upcgray}{\textbf{Problems:}}
\begin{itemize}
    \tiny
    \item Classes overlap significantly
    \item Linear projection limitations
    \item Poor cluster separation
\end{itemize}
\end{column}

\begin{column}{0.48\textwidth}
\textbf{\color{upcblue}t-SNE on MNIST Digits}

\begin{center}
\begin{tikzpicture}[scale=0.6]
    % t-SNE well-separated clusters
    \foreach \x/\y/\c/\l in {-1.1/0.9/red!70/0, 1.2/0.7/blue!70/1, 
                              -0.7/-1/green!70/2, 0.9/-0.9/orange!70/3, 
                              0/0.2/purple!70/4} {
        % Tight cluster
        \foreach \i in {1,...,5} {
            \pgfmathsetmacro{\angle}{360*\i/5}
            \node[circle, fill=\c, inner sep=1.3pt] at ({\x+0.12*cos(\angle)},{\y+0.12*sin(\angle)}) {};
        }
        \node[circle, fill=\c, inner sep=1.3pt] at (\x,\y) {};
        \node at (\x,\y-0.35) {\tiny \l};
    }
    
    \node at (0,-2.2) {\tiny \textbf{t-SNE: Clear Separation}};
\end{tikzpicture}
\end{center}

\footnotesize
\textcolor{highlight}{\textbf{Advantages:}}
\begin{itemize}
    \tiny
    \item Distinct clusters
    \item Preserves local structure
    \item Reveals true relationships
\end{itemize}
\end{column}
\end{columns}

\vspace{0.15cm}
\begin{center}
\colorbox{upcblue!10}{
\begin{minipage}{0.8\textwidth}
\centering
\footnotesize\textit{``t-SNE focuses on preserving \textbf{local neighborhood structure}''}\\
\footnotesize\textit{Similar points in high-D $\rightarrow$ nearby points in low-D}
\end{minipage}
}
\end{center}
\end{frame}

% SLIDE 8: Lecture Outline (Aesthetic Revision)
\begin{frame}{Today's Journey: From Theory to Mastery}
\vspace{1cm} 
\centering
\begin{tikzpicture}[
    scale=0.9, transform shape,
    % Styles
    box/.style={
        draw, thick, rounded corners,
        minimum width=3.6cm, minimum height=3.8cm,
        align=center, font=\footnotesize,
        inner sep=5pt
    },
    point/.style={
        circle, fill, minimum size=8pt, inner sep=0pt
    },
    timeline/.style={
        -latex, thick, upcgray!80, line cap=round
    }
]

% The main timeline arrow
\draw[timeline] (-6.5,0) -- (6.5,0);

% Part 1: SNE
\node[point, fill=upcblue] (p1) at (-5,0) {};
\node[box, draw=upcblue, fill=upcblue!10, above=0.5cm of p1] (b1) {
    \textbf{\color{upcblue}Part 1: SNE} \\
    \small(Foundation) \\ \vspace{2mm}
    Gaussian Kernels \\
    KL Divergence \\
    Symmetric SNE \\
    Limitations
};
\draw[->, thick, upcblue!80] (p1.north) -- (b1.south);

% Part 2: t-SNE
\node[point, fill=highlight] (p2) at (0,0) {};
\node[box, draw=highlight, fill=highlight!10, above=0.5cm of p2] (b2) {
    \textbf{\color{highlight}Part 2: t-SNE} \\
    \small(Innovation) \\ \vspace{2mm}
    Crowding Problem \\
    Student's t-dist. \\
    Heavy Tails \\
    Optimization
};
\draw[->, thick, highlight!80] (p2.north) -- (b2.south);

% Part 3: Practice
\node[point, fill=upcgray] (p3) at (5,0) {};
\node[box, draw=upcgray, fill=upcgray!10, above=0.5cm of p3] (b3) {
    \textbf{\color{upcgray}Part 3: Practice} \\
    \small(Application) \\ \vspace{2mm}
    Perplexity \& LR \\
    Interpretation \\
    Common Pitfalls \\
    t-SNE vs UMAP
};
\draw[->, thick, upcgray!80] (p3.north) -- (b3.south);

\end{tikzpicture}
\end{frame}


% SLIDE 9: Let's Begin - The Core Idea
\begin{frame}{From Distances to Probabilities: The Core Idea}
\vspace{-0.2cm}

\begin{center}
\colorbox{upcblue!10}{
\begin{minipage}{0.85\textwidth}
\centering
\textbf{Central Insight:} Convert distances between points into probabilities\\
that represent neighborhood relationships
\end{minipage}
}
\end{center}

\vspace{0.3cm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{\color{upcblue}Traditional Approach}
\vspace{0.2cm}

\begin{tikzpicture}[scale=0.7]
    % Distance-based
    \node[circle, fill=upcblue, inner sep=2pt] (c) at (0,0) {};
    \node[below] at (c) {\tiny $\mathbf{x}_i$};
    
    \node[circle, fill=upcgray, inner sep=2pt] (p1) at (1,0.5) {};
    \node[right] at (p1) {\tiny $d_{ij}$};
    
    \node[circle, fill=upcgray, inner sep=2pt] (p2) at (0.8,-0.7) {};
    \node[right] at (p2) {\tiny $d_{ik}$};
    
    \node[circle, fill=upcgray, inner sep=2pt] (p3) at (-1.2,0.3) {};
    \node[left] at (p3) {\tiny $d_{il}$};
    
    \draw[dashed, upcgray] (c) -- (p1);
    \draw[dashed, upcgray] (c) -- (p2);
    \draw[dashed, upcgray] (c) -- (p3);
    
    \node at (0,-1.5) {\footnotesize \textbf{Euclidean Distances}};
\end{tikzpicture}

\footnotesize
Problem: How to weight different distances?
\end{column}

\begin{column}{0.48\textwidth}
\textbf{\color{upcblue}SNE/t-SNE Approach}
\vspace{0.2cm}

\begin{tikzpicture}[scale=0.7]
    % Probability-based
    \node[circle, fill=highlight, inner sep=2pt] (c) at (0,0) {};
    \node[below] at (c) {\tiny $\mathbf{x}_i$};
    
    \node[circle, fill=upcgray, inner sep=2pt] (p1) at (1,0.5) {};
    \node[right] at (p1) {\tiny $p_{j|i}$};
    
    \node[circle, fill=upcgray, inner sep=2pt] (p2) at (0.8,-0.7) {};
    \node[right] at (p2) {\tiny $p_{k|i}$};
    
    \node[circle, fill=upcgray, inner sep=2pt] (p3) at (-1.2,0.3) {};
    \node[left] at (p3) {\tiny $p_{l|i}$};
    
    \draw[->, highlight, thick] (c) -- (p1);
    \draw[->, highlight, thick] (c) -- (p2);
    \draw[->, highlight, thick] (c) -- (p3);
    
    \node at (0,-1.5) {\footnotesize \textbf{Probabilities}};
\end{tikzpicture}

\footnotesize
Solution: Probabilities naturally normalize!
\end{column}
\end{columns}

\vspace{0.3cm}
\begin{center}
\colorbox{highlight!20}{
\begin{minipage}{0.85\textwidth}
\centering
\footnotesize\textit{``What is the probability that point $i$ would pick point $j$ as its neighbor?''}
\end{minipage}
}
\end{center}
\end{frame}



% SLIDE 10: Part 1 Begins - SNE Foundations
\begin{frame}{Part 1: Stochastic Neighbor Embedding (SNE)}
\vspace{-0.2cm}

\begin{center}
\colorbox{upcblue!10}{
\begin{minipage}{0.85\textwidth}
\centering
\textbf{The Foundation:} Understanding the original SNE algorithm\\
before moving to t-SNE improvements
\end{minipage}
}
\end{center}

\vspace{0.3cm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{\color{upcblue}What We'll Cover}
\vspace{0.2cm}

\begin{enumerate}
    \small
    \setlength\itemsep{0.3em}
    \item High-dimensional similarities
    \item Gaussian kernels
    \item Conditional probabilities
    \item Perplexity parameter
    \item Low-dimensional mapping
    \item KL divergence objective
    \item Gradient computation
\end{enumerate}
\end{column}

\begin{column}{0.48\textwidth}
\textbf{\color{upcblue}Key References}
\vspace{0.2cm}

\small
\textcolor{upcgray}{Original Paper:}\\
\footnotesize
Hinton \& Roweis (2002)\\
\textit{``Stochastic Neighbor Embedding''}\\
NIPS 2002

\vspace{0.3cm}
\textcolor{upcgray}{Mathematical Framework:}\\
\footnotesize
Building on MDS, Isomap, LLE\\
but with probabilistic approach

\vspace{0.3cm}
\begin{center}
\begin{tikzpicture}[scale=0.6]
    \draw[thick, upcblue] (0,0) circle (0.8cm);
    \node at (0,0) {\footnotesize SNE};
    \draw[->, thick, upcgray] (0.9,0) -- (1.5,0);
    \draw[thick, highlight] (2.3,0) circle (0.8cm);
    \node at (2.3,0) {\footnotesize t-SNE};
\end{tikzpicture}
\end{center}
\end{column}
\end{columns}

\vspace{0.3cm}
\begin{center}
\colorbox{highlight!20}{
\begin{minipage}{0.85\textwidth}
\centering
\footnotesize\textit{``SNE converts high-dimensional Euclidean distances into}\\
\footnotesize\textit{conditional probabilities that represent similarities''}
\end{minipage}
}
\end{center}
\end{frame}




% SLIDE 11: The Core Challenge - Defining 'Neighborhood'
\begin{frame}{The Challenge of Defining 'Neighborhood'}
\vspace{-0.2cm}

\begin{center}
\textbf{\color{upcblue}A Fundamental Question:} When are two points "neighbors"?
\end{center}

\vspace{0.3cm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{\color{upcblue}Dense Region}
\begin{center}
\begin{tikzpicture}[scale=0.8]
    % Dense cluster
    \node[circle, fill=red, inner sep=2pt] at (0,0) {};
    \node[below] at (0,-0.2) {\tiny A};
    \node[circle, fill=red, inner sep=2pt] at (0.7,0) {};
    \node[below] at (0.7,-0.2) {\tiny B};
    
    % Other nearby points
    \foreach \angle in {30,60,120,150,210,240,300,330} {
        \node[circle, fill=upcgray!60, inner sep=1pt] at ({0.35+0.4*cos(\angle)},{0.4*sin(\angle)}) {};
    }
    
    \draw[<->, thick] (0.1,0.15) -- (0.6,0.15);
    \node at (0.35,0.35) {\tiny 1 unit};
    
    \node at (0,-1.5) {\footnotesize Many neighbors nearby};
\end{tikzpicture}
\end{center}
\end{column}

\begin{column}{0.48\textwidth}
\textbf{\color{upcblue}Sparse Region}
\begin{center}
\begin{tikzpicture}[scale=0.8]
    % Sparse points
    \node[circle, fill=blue, inner sep=2pt] at (0,0) {};
    \node[below] at (0,-0.2) {\tiny C};
    \node[circle, fill=blue, inner sep=2pt] at (0.7,0) {};
    \node[below] at (0.7,-0.2) {\tiny D};
    
    % Few distant points
    \node[circle, fill=upcgray!40, inner sep=1pt] at (1.8,0.5) {};
    \node[circle, fill=upcgray!40, inner sep=1pt] at (-1.5,-0.6) {};
    \node[circle, fill=upcgray!40, inner sep=1pt] at (0.3,1.4) {};
    
    \draw[<->, thick] (0.1,0.15) -- (0.6,0.15);
    \node at (0.35,0.35) {\tiny 1 unit};
    
    \node at (0,-1.5) {\footnotesize Few neighbors nearby};
\end{tikzpicture}
\end{center}
\end{column}
\end{columns}

\vspace{0.3cm}
\begin{center}
\colorbox{highlight!20}{
\begin{minipage}{0.85\textwidth}
\centering
\textbf{The Problem:} Should A-B have the same "similarity" as C-D?\\
\footnotesize Both are 1 unit apart, but contexts are completely different!
\end{minipage}
}
\end{center}

\vspace{0.2cm}
\begin{center}
\textit{\color{upcblue}We need similarity to be \textbf{relative to local density}}
\end{center}
\end{frame}

% SLIDE 12: Step 1 - From Distance to Similarity
\begin{frame}{Tool 1: The Gaussian Kernel}
\vspace{-0.2cm}

\textbf{\color{upcblue}Building Our Similarity Measure Step by Step}

\vspace{0.2cm}
\begin{center}
\textbf{Step 1:} Start with squared Euclidean distance
$$d_{ij}^2 = \|\mathbf{x}_i - \mathbf{x}_j\|^2$$
\end{center}

\vspace{0.2cm}
\begin{center}
\textbf{Step 2:} Convert distance to similarity (closer = more similar)
$$\text{sim} = \exp(-d_{ij}^2)$$

\begin{tikzpicture}[scale=0.7]
    \draw[->] (0,0) -- (4,0) node[right] {\footnotesize distance};
    \draw[->] (0,0) -- (0,2) node[above] {\footnotesize similarity};
    \draw[thick, upcblue, domain=0:3.5] plot (\x, {1.8*exp(-\x/2)});
    \node at (2,-0.5) {\footnotesize Exponential decay};
\end{tikzpicture}
\end{center}

\vspace{0.2cm}
\begin{center}
\textbf{Step 3:} Add bandwidth control with $\sigma_i$ 
$$\text{sim} = \exp\left(-\frac{\|\mathbf{x}_i - \mathbf{x}_j\|^2}{2\sigma_i^2}\right)$$

\begin{tikzpicture}[scale=0.6]
    \draw[->] (0,0) -- (4,0) node[right] {\tiny distance};
    \draw[->] (0,0) -- (0,2) node[above] {\tiny similarity};
    
    \draw[thick, red, domain=0:3.5] plot (\x, {1.8*exp(-2*\x)});
    \node[red] at (1.5,0.8) {\tiny small $\sigma$};
    
    \draw[thick, blue, domain=0:3.5] plot (\x, {1.8*exp(-0.3*\x)});
    \node[blue] at (3,1.2) {\tiny large $\sigma$};
\end{tikzpicture}
\end{center}

\begin{center}
\colorbox{upcblue!10}{
\begin{minipage}{0.85\textwidth}
\centering
\footnotesize $\sigma_i$ is our "tuning knob" - it adapts to local density!
\end{minipage}
}
\end{center}
\end{frame}

% SLIDE 13: Step 2 - The Normalization Problem
\begin{frame}{From Similarity Scores to Probabilities}
\vspace{-0.2cm}

\textbf{\color{upcblue}The Next Problem:} Our similarity scores aren't probabilities!

\vspace{0.3cm}
\begin{center}
\begin{tikzpicture}[scale=0.9]
    % Point i and its similarities
    \node[circle, fill=upcblue, inner sep=3pt] at (0,0) {};
    \node[below] at (0,-0.2) {\footnotesize $\mathbf{x}_i$};
    
    % Neighbors with similarity scores
    \node[circle, fill=upcgray, inner sep=2pt] at (1.5,0.8) {};
    \node[right] at (1.7,0.8) {\footnotesize sim = 0.8};
    
    \node[circle, fill=upcgray, inner sep=2pt] at (2,0) {};
    \node[right] at (2.2,0) {\footnotesize sim = 0.6};
    
    \node[circle, fill=upcgray, inner sep=2pt] at (1.2,-0.9) {};
    \node[right] at (1.4,-0.9) {\footnotesize sim = 0.4};
    
    \node[circle, fill=upcgray, inner sep=2pt] at (-1.5,0.5) {};
    \node[left] at (-1.7,0.5) {\footnotesize sim = 0.3};
    
    \node[circle, fill=upcgray, inner sep=2pt] at (-1,-1) {};
    \node[left] at (-1.2,-1) {\footnotesize sim = 0.2};
    
    \draw[->, red, thick] (3,0) -- (4,0);
    
    \node at (5.5,0) {\footnotesize Sum = 2.3 $\neq$ 1};
\end{tikzpicture}
\end{center}

\vspace{0.3cm}
\begin{center}
\colorbox{highlight!20}{
\begin{minipage}{0.85\textwidth}
\centering
\textbf{Why is this a problem?}\\
\footnotesize
- We want to interpret values as probabilities\\
- Probabilities must sum to 1\\
- Need consistent scale across all points
\end{minipage}
}
\end{center}

\vspace{0.2cm}
\begin{center}
\textit{\color{upcblue}Question: How do we normalize these scores?}
\end{center}
\end{frame}

% SLIDE 14: Step 3 - The Solution (Softmax)
\begin{frame}{Tool 2: Softmax for Normalization}
\vspace{-0.2cm}

\textbf{\color{upcblue}The Solution:} Divide by the sum of all similarities!

\vspace{0.3cm}
\begin{center}
\begin{tikzpicture}[scale=0.9]
    % Normalization formula visualization
    \node[draw, rounded corners, fill=upcblue!10, minimum width=6cm, minimum height=1.5cm] at (0,0) {
        \begin{minipage}{5.8cm}
        \centering
        $$p_{j|i} = \frac{\text{similarity to } j}{\text{sum of all similarities}}$$
        \end{minipage}
    };
\end{tikzpicture}
\end{center}

\vspace{0.3cm}
\textbf{The Complete Formula:}
\begin{center}
\colorbox{upcblue!10}{
\begin{minipage}{0.9\textwidth}
\centering
$$p_{j|i} = \frac{\exp(-\|\mathbf{x}_i - \mathbf{x}_j\|^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-\|\mathbf{x}_i - \mathbf{x}_k\|^2 / 2\sigma_i^2)}$$
\end{minipage}
}
\end{center}

\vspace{0.3cm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{This is the Softmax!}
\footnotesize
- Standard tool in ML\\
- Converts scores to probabilities\\
- Preserves relative magnitudes\\
- Ensures $\sum_j p_{j|i} = 1$
\end{column}

\begin{column}{0.48\textwidth}
\textbf{Interpretation:}
\footnotesize
$p_{j|i}$ = probability that point $i$\\
would pick point $j$ as its neighbor\\
if picking based on similarity
\end{column}
\end{columns}

\vspace{0.2cm}
\begin{center}
\colorbox{highlight!20}{
\begin{minipage}{0.85\textwidth}
\centering
\footnotesize Now we have valid probability distributions, but still need to set each $\sigma_i$!
\end{minipage}
}
\end{center}
\end{frame}

% SLIDE 15: The Perplexity Puzzle
\begin{frame}{An Impossible Task: Setting Each $\sigma_i$}
\vspace{-0.2cm}

\textbf{\color{upcblue}The Remaining Challenge:} How to choose $\sigma_i$ for every point?

\vspace{0.3cm}
\begin{center}
\begin{tikzpicture}[scale=0.8]
    % Dataset with many points
    \foreach \i in {1,...,30} {
        \pgfmathsetmacro{\rx}{4*rand}
        \pgfmathsetmacro{\ry}{2*rand}
        \node[circle, fill=upcgray!40, inner sep=1pt] at (\rx,\ry) {};
    }
    
    % Highlight a few points
    \node[circle, fill=red, inner sep=2pt] at (1,0.5) {};
    \node at (1,0.2) {\tiny $\sigma_1 = ?$};
    
    \node[circle, fill=blue, inner sep=2pt] at (3,1.5) {};
    \node at (3,1.2) {\tiny $\sigma_2 = ?$};
    
    \node[circle, fill=green, inner sep=2pt] at (0.5,1.8) {};
    \node at (0.5,1.5) {\tiny $\sigma_3 = ?$};
    
    \node at (2,-0.5) {\footnotesize ... thousands of points!};
\end{tikzpicture}
\end{center}

\vspace{0.3cm}
\begin{center}
\colorbox{red!10}{
\begin{minipage}{0.85\textwidth}
\centering
\textbf{Problems with manual setting:}\\
\footnotesize
- Need different $\sigma_i$ for each point (adapts to density)\\
- Dataset might have thousands of points\\
- No intuitive way to choose values\\
- Trial and error is impractical
\end{minipage}
}
\end{center}

\vspace{0.3cm}
\begin{center}
\textit{\color{upcblue}We need a more intuitive, global parameter!}
\end{center}
\end{frame}

% SLIDE 16: The Solution - Perplexity
\begin{frame}{A Better Knob: Perplexity}
\vspace{-0.2cm}

\textbf{\color{upcblue}The Elegant Solution:} Specify the "effective number of neighbors"!

\vspace{0.3cm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{Perplexity Definition:}
$$\text{Perp}(P_i) = 2^{H(P_i)}$$

where entropy:
$$H(P_i) = -\sum_j p_{j|i} \log_2 p_{j|i}$$

\vspace{0.2cm}
\textbf{Intuition:}\\
\footnotesize
"I want each point to have\\
about 30 effective neighbors"\\
$\Rightarrow$ Set Perplexity = 30
\end{column}

\begin{column}{0.48\textwidth}
\textbf{The Algorithm:}
\begin{center}
\begin{tikzpicture}[scale=0.6]
    \node[draw, rounded corners, fill=upcblue!10] at (0,0) {\footnotesize User sets Perp};
    \draw[->, thick] (0,-0.5) -- (0,-1.5);
    
    \node[draw, rounded corners, fill=highlight!10] at (0,-2) {\footnotesize For each point $i$:};
    \draw[->, thick] (0,-2.5) -- (0,-3.5);
    
    \node[draw, rounded corners, fill=upcgray!10] at (0,-4) {
        \begin{minipage}{3cm}
        \centering
        \tiny Binary search to find\\
        \tiny $\sigma_i$ that achieves\\
        \tiny target perplexity
        \end{minipage}
    };
\end{tikzpicture}
\end{center}
\end{column}
\end{columns}

\vspace{0.2cm}
\begin{center}
\begin{tikzpicture}[scale=0.8]
    % Visual effect
    \node at (-3,0) {\footnotesize Dense region};
    \node[circle, fill=upcblue, inner sep=1.5pt] at (-3,-0.5) {};
    \draw[upcblue, thick] (-3,-0.5) circle (0.3cm);
    \node at (-3,-1) {\tiny auto: small $\sigma$};
    
    \node at (0,0) {\footnotesize Medium density};
    \node[circle, fill=upcblue, inner sep=1.5pt] at (0,-0.5) {};
    \draw[upcblue, thick] (0,-0.5) circle (0.5cm);
    \node at (0,-1) {\tiny auto: medium $\sigma$};
    
    \node at (3,0) {\footnotesize Sparse region};
    \node[circle, fill=upcblue, inner sep=1.5pt] at (3,-0.5) {};
    \draw[upcblue, thick] (3,-0.5) circle (0.8cm);
    \node at (3,-1) {\tiny auto: large $\sigma$};
\end{tikzpicture}
\end{center}

\begin{center}
\colorbox{upcblue!10}{
\begin{minipage}{0.85\textwidth}
\centering
\footnotesize\textbf{Result:} One intuitive parameter (Perplexity) automatically sets all $\sigma_i$ values!
\end{minipage}
}
\end{center}
\end{frame}

% SLIDE 17: Low-Dimensional Similarities
\begin{frame}{Mapping to the Low-D Space: q(j|i)}
\vspace{-0.2cm}

\textbf{\color{upcblue}The Next Step:} Define similarities in the map space

\vspace{0.3cm}
\begin{center}
\colorbox{upcblue!10}{
\begin{minipage}{0.85\textwidth}
\centering
Similar formula, but with a crucial difference:
$$q_{j|i} = \frac{\exp(-\|\mathbf{y}_i - \mathbf{y}_j\|^2)}{\sum_{k \neq i} \exp(-\|\mathbf{y}_i - \mathbf{y}_k\|^2)}$$
\end{minipage}
}
\end{center}

\vspace{0.3cm}
\textbf{Key Difference: Fixed variance!}

\begin{columns}[T]
\begin{column}{0.48\textwidth}
\begin{center}
\textbf{High-D Space}\\
\begin{tikzpicture}[scale=0.6]
    \node[circle, fill=upcblue, inner sep=2pt] at (0,0) {};
    \draw[upcblue, dashed] (0,0) circle (0.4cm);
    \node[circle, fill=upcblue, inner sep=2pt] at (2,0) {};
    \draw[upcblue, dashed] (2,0) circle (0.8cm);
    
    \node at (1,-1.2) {\tiny Varying $\sigma_i$};
\end{tikzpicture}
\end{center}
\end{column}

\begin{column}{0.48\textwidth}
\begin{center}
\textbf{Low-D Space}\\
\begin{tikzpicture}[scale=0.6]
    \node[circle, fill=highlight, inner sep=2pt] at (0,0) {};
    \draw[highlight, dashed] (0,0) circle (0.5cm);
    \node[circle, fill=highlight, inner sep=2pt] at (2,0) {};
    \draw[highlight, dashed] (2,0) circle (0.5cm);
    
    \node at (1,-1.2) {\tiny Fixed $\sigma = 1/\sqrt{2}$};
\end{tikzpicture}
\end{center}
\end{column}
\end{columns}

\vspace{0.3cm}
\begin{center}
\colorbox{highlight!20}{
\begin{minipage}{0.85\textwidth}
\centering
\textbf{Why fixed variance?}\\
\footnotesize
- Encourages uniform distribution in visualization\\
- Prevents all points from collapsing together\\
- Scale of map is arbitrary anyway
\end{minipage}
}
\end{center}
\end{frame}

% SLIDE 18: The Grand Objective
\begin{frame}{The Objective: Aligning P and Q}
\vspace{-0.2cm}

\textbf{\color{upcblue}Where We Are:}

\vspace{0.3cm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\begin{center}
\textbf{High-D Space}\\
\vspace{0.2cm}
\begin{tikzpicture}[scale=0.7]
    \draw[fill=upcblue!10] (-1.5,-1.5) rectangle (1.5,1.5);
    \node at (0,1.8) {\footnotesize Data points $\mathbf{x}_i$};
    
    \node[circle, fill=upcblue, inner sep=2pt] at (0,0) {};
    \foreach \angle/\p in {30/0.3, 90/0.2, 150/0.15, 210/0.1, 270/0.15, 330/0.1} {
        \node[circle, fill=upcgray, inner sep=1pt] at ({cos(\angle)},{sin(\angle)}) {};
        \draw[->, thick] (0,0) -- ({0.5*cos(\angle)},{0.5*sin(\angle)});
    }
    
    \node at (0,-1.8) {\footnotesize Distributions $P_i$};
    \node at (0,-2.1) {\tiny (fixed by data)};
\end{tikzpicture}
\end{center}
\end{column}

\begin{column}{0.48\textwidth}
\begin{center}
\textbf{Low-D Space}\\
\vspace{0.2cm}
\begin{tikzpicture}[scale=0.7]
    \draw[fill=highlight!10] (-1.5,-1.5) rectangle (1.5,1.5);
    \node at (0,1.8) {\footnotesize Map points $\mathbf{y}_i$};
    
    \node[circle, fill=highlight, inner sep=2pt] at (0,0) {};
    \foreach \angle/\p in {45/0.1, 100/0.3, 180/0.2, 240/0.05, 300/0.25, 350/0.1} {
        \node[circle, fill=upcgray, inner sep=1pt] at ({cos(\angle)},{sin(\angle)}) {};
        \draw[->, thick, red] (0,0) -- ({0.5*cos(\angle)},{0.5*sin(\angle)});
    }
    
    \node at (0,-1.8) {\footnotesize Distributions $Q_i$};
    \node at (0,-2.1) {\tiny (we optimize)};
\end{tikzpicture}
\end{center}
\end{column}
\end{columns}

\vspace{0.3cm}
\begin{center}
\colorbox{upcblue!10}{
\begin{minipage}{0.85\textwidth}
\centering
\textbf{The Goal:} Arrange points $\mathbf{y}_i$ so that $Q_i \approx P_i$ for all $i$
\end{minipage}
}
\end{center}

\vspace{0.2cm}
\begin{center}
\textit{\color{upcblue}Question: How do we measure the total "mismatch"?}
\end{center}
\end{frame}

% SLIDE 19: Measuring Mismatch - KL Divergence
\begin{frame}{Tool 3: Kullback-Leibler Divergence}
\vspace{-0.2cm}

\textbf{\color{upcblue}Measuring Distribution Mismatch:} KL Divergence

\vspace{0.3cm}
\begin{center}
\colorbox{upcblue!10}{
\begin{minipage}{0.85\textwidth}
\centering
$$\text{KL}(P||Q) = \sum_j p_j \log\left(\frac{p_j}{q_j}\right)$$
\footnotesize "The cost of encoding data from P using a code optimized for Q"
\end{minipage}
}
\end{center}

\vspace{0.3cm}
\textbf{Critical Property: Asymmetry!}

\begin{center}
\begin{tikzpicture}[scale=0.9]
    % Case illustrations
    \node at (-3,1.5) {\footnotesize \textbf{Case 1: $p_j$ large, $q_j$ small}};
    \draw[fill=upcblue!60] (-3.5,0.5) rectangle (-3.3,1);
    \node at (-3.4,0.2) {\tiny $p_j$};
    \draw[fill=highlight!30] (-2.8,0.5) rectangle (-2.6,0.6);
    \node at (-2.7,0.2) {\tiny $q_j$};
    
    \node at (-3,-0.2) {\footnotesize $\log(p_j/q_j)$ is \textcolor{red}{large positive}};
    \node at (-3,-0.5) {\footnotesize $\Rightarrow$ \textcolor{red}{High penalty!}};
    
    \node at (3,1.5) {\footnotesize \textbf{Case 2: $p_j$ small, $q_j$ large}};
    \draw[fill=upcblue!30] (2.5,0.5) rectangle (2.7,0.6);
    \node at (2.6,0.2) {\tiny $p_j$};
    \draw[fill=highlight!60] (3.2,0.5) rectangle (3.4,1);
    \node at (3.3,0.2) {\tiny $q_j$};
    
    \node at (3,-0.2) {\footnotesize $\log(p_j/q_j)$ is negative};
    \node at (3,-0.5) {\footnotesize $\Rightarrow$ \textcolor{orange}{Small penalty}};
\end{tikzpicture}
\end{center}

\vspace{0.2cm}
\begin{center}
\colorbox{highlight!20}{
\begin{minipage}{0.85\textwidth}
\centering
\footnotesize\textbf{Implication:} Algorithm focuses on preserving local structure\\
(high penalty for separating neighbors, low penalty for collapsing non-neighbors)
\end{minipage}
}
\end{center}
\end{frame}

% SLIDE 20: The SNE Cost Function
\begin{frame}{The SNE Cost Function: C}
\vspace{-0.2cm}

\textbf{\color{upcblue}Putting It All Together:} The Complete Objective

\vspace{0.3cm}
\begin{center}
\colorbox{upcblue!10}{
\begin{minipage}{0.85\textwidth}
\centering
$$C = \sum_{i=1}^n \text{KL}(P_i || Q_i) = \sum_i \sum_j p_{j|i} \log\frac{p_{j|i}}{q_{j|i}}$$
\end{minipage}
}
\end{center}

\vspace{0.3cm}
\textbf{What This Means:}

\begin{center}
\begin{tikzpicture}[scale=0.9]
    % Visualization of the optimization
    \node[draw, rounded corners, fill=upcblue!10, minimum width=3cm, minimum height=1cm] at (-3,0) {
        \begin{minipage}{2.8cm}
        \centering
        \footnotesize
        Random initial\\
        positions $\mathbf{y}_i$
        \end{minipage}
    };
    
    \draw[->, thick, upcblue] (-1.3,0) -- (-0.3,0);
    
    \node[draw, rounded corners, fill=highlight!10, minimum width=3cm, minimum height=1cm] at (1.5,0) {
        \begin{minipage}{2.8cm}
        \centering
        \footnotesize
        Gradient\\
        descent
        \end{minipage}
    };
    
    \draw[->, thick, highlight] (3.2,0) -- (4.2,0);
    
    \node[draw, rounded corners, fill=green!10, minimum width=3cm, minimum height=1cm] at (6,0) {
        \begin{minipage}{2.8cm}
        \centering
        \footnotesize
        Optimized\\
        map
        \end{minipage}
    };
    
    \node at (1.5,-1.5) {\footnotesize Minimize $C$ by adjusting $\mathbf{y}_i$ positions};
\end{tikzpicture}
\end{center}

\vspace{0.3cm}
\begin{center}
\colorbox{highlight!20}{
\begin{minipage}{0.85\textwidth}
\centering
\footnotesize\textbf{The asymmetric penalty means:}\\
- Similar points (high $p_{j|i}$) mapped far apart $\Rightarrow$ Large cost\\
- Dissimilar points (low $p_{j|i}$) mapped close $\Rightarrow$ Small cost\\
$\therefore$ SNE preserves local neighborhoods!
\end{minipage}
}
\end{center}
\end{frame}

\end{document}