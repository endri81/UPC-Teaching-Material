\documentclass[aspectratio=169]{beamer}
\usetheme{Madrid}
\usecolortheme{seahorse}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{pgfplots}     % This automatically loads the 'tikz' package
\pgfplotsset{compat=1.18} % Good practice for pgfplots
\usepackage{amsmath}      % For the 'align*' environment
\usetikzlibrary{shapes.geometric, arrows.meta, positioning, calc, patterns}

\pgfplotsset{compat=1.17}

% Custom colors
\definecolor{highDcolor}{RGB}{46, 134, 171}
\definecolor{lowDcolor}{RGB}{162, 59, 114}
\definecolor{gradcolor}{RGB}{255, 127, 14}

% Add these color definitions in the preamble (after line 16)
\definecolor{gray30}{gray}{0.3}
\definecolor{gray50}{gray}{0.5}
\definecolor{gray70}{gray}{0.7}

% Custom commands
\newcommand{\conceptbox}[2]{\colorbox{#1!20}{\textcolor{#1}{\textbf{#2}}}}
\newcommand{\warning}[1]{\conceptbox{red}{Warning: #1}}
\newcommand{\insight}[1]{\conceptbox{blue}{Insight: #1}}
\newcommand{\ethics}[1]{\conceptbox{purple}{Ethics: #1}}

\title{t-Stochastic Neighbor Embedding}
\subtitle{Complete 80-Slide Presentation}
\author{Prof.Asc. Endri Raco}
\institute{Polytechnic University of Tirane}
\date{October 2025}

\begin{document}

% ============================================
% SLIDES 1-10: Introduction and Fundamentals
% ============================================

% Slide 1
\begin{frame}
\titlepage
\end{frame}

\begin{frame}{What is Dimensionality Reduction?}
\begin{block}{Definition}
Transforming high-dimensional data into lower-dimensional representations while preserving meaningful structure
\end{block}

\textbf{Why We Need It:}
\begin{itemize}
\item Visualization: Human perception limited to 3D
\item Curse of dimensionality: Distance becomes meaningless in high-D
\item Computational efficiency: Reduce processing requirements
\item Feature extraction: Identify essential patterns
\end{itemize}

\textbf{The Central Challenge:}
\begin{center}
\Large How do we decide what to preserve when we must lose information?
\end{center}

\begin{columns}
\column{0.5\textwidth}
\centering
Traditional answer: \textcolor{blue}{Preserve distances}

\column{0.5\textwidth}
\centering
t-SNE answer: \textcolor{red}{Preserve neighborhoods}
\end{columns}
\end{frame}

% Slide 2
\begin{frame}{The Fundamental Challenge of Dimensionality Reduction}
\begin{columns}
\column{0.5\textwidth}
\begin{center}
\begin{tikzpicture}[scale=1.2]
\shade[ball color=highDcolor!40] (0,0) circle (2cm);
\foreach \angle in {0,20,...,340} {
    \pgfmathsetmacro\radius{1.7+0.2*rnd}
    \fill[white!70!highDcolor] (\angle:\radius) circle (2pt);
}
\node at (0,-2.5) {\textbf{784 Dimensions}};
\node at (0,-3) {\small MNIST digit};
\end{tikzpicture}
\end{center}

\column{0.5\textwidth}
\begin{center}
\begin{tikzpicture}[scale=1.2]
\draw[thick] (-1.5,-1.5) rectangle (1.5,1.5);
\foreach \i in {1,...,30} {
    \pgfmathsetmacro\x{0.3*rnd}
    \pgfmathsetmacro\y{0.3*rnd}
    \fill[lowDcolor] (\x,\y) circle (2pt);
}
\node at (0,-2.5) {\textbf{2 Dimensions}};
\node at (0,-3) {\small Your screen};
\end{tikzpicture}
\end{center}
\end{columns}

\vspace{0.5cm}
\begin{center}
\Large\textbf{How do we preserve neighborhood relationships \\ when destroying geometric structure?}
\end{center}
\end{frame}


% Slide 3
\begin{frame}{The Crowding Problem: Why Linear Methods Fail}
\begin{block}{Definition}
\textbf{Crowding Problem:} The geometric impossibility of preserving moderate-range distances when projecting from high to low dimensions, causing distinct distance scales to collapse.
\end{block}

\begin{columns}
\column{0.5\textwidth}
\begin{center}
\textbf{High-D Space (10D)}\\[0.2cm]
\begin{tikzpicture}[scale=1.0]
% Center point
\fill[black] (0,0) circle (3pt);
\node at (0,-0.35) {\footnotesize Point A};

% Near neighbors (distance = 1)
\foreach \angle in {0,60,120,180,240,300} {
    \fill[green!80] (\angle:0.7) circle (2.5pt);
}
\draw[green, very thick] (0,0) circle (0.7);
\node[green] at (1.1,0.6) {\footnotesize Near};
\node[green] at (1.1,0.3) {\tiny $d=1$};

% Moderate neighbors (distance = 5)
\foreach \angle in {30,90,150,210,270,330} {
    \fill[orange!80] (\angle:1.4) circle (2.5pt);
}
\draw[orange, very thick, dashed] (0,0) circle (1.4);
\node[orange] at (1.8,0.6) {\footnotesize Med};
\node[orange] at (1.8,0.3) {\tiny $d=5$};

% Far neighbors (distance = 10)
\foreach \angle in {15,75,135,195,255,315} {
    \fill[red!80] (\angle:2.1) circle (2.5pt);
}
\draw[red, very thick, dotted] (0,0) circle (2.1);
\node[red] at (2.5,0.6) {\footnotesize Far};
\node[red] at (2.5,0.3) {\tiny $d=10$};

\node at (0,-2.7) {\small\textbf{Ratio: 1 : 5 : 10}};
\end{tikzpicture}
\end{center}

\column{0.5\textwidth}
\begin{center}
\textbf{After Linear Projection to 2D}\\[0.2cm]
\begin{tikzpicture}[scale=1.0]
% Center point
\fill[black] (0,0) circle (3pt);
\node at (0,-0.35) {\footnotesize Point A};

% All points crushed into narrow ring
\draw[gray, very thick, fill=gray!20] (0,0) circle (0.6);
\draw[gray, very thick, fill=white] (0,0) circle (0.4);

% Mixed points in narrow annulus
\foreach \i in {1,...,18} {
    \pgfmathsetmacro\r{0.4 + 0.2*rnd}
    \pgfmathsetmacro\a{20*\i + 10*rnd}
    \pgfmathsetmacro\colorChoice{int(rnd*3)}
    \ifnum\colorChoice=0
        \fill[green!80] (\a:\r) circle (2pt);
    \fi
    \ifnum\colorChoice=1
        \fill[orange!80] (\a:\r) circle (2pt);
    \fi
    \ifnum\colorChoice=2
        \fill[red!80] (\a:\r) circle (2pt);
    \fi
}

% Show compressed distances
\draw[<->, thick] (0.4,0) -- (0.6,0);
\node at (0.5,-0.2) {\tiny 0.2};

\node at (0,-2.7) {\small\textbf{\textcolor{red}{Ratio: 1 : 1.1 : 1.2}}};
\end{tikzpicture}
\end{center}
\end{columns}

\vspace{0.15cm}
\textbf{Key Insight:} Available "area" in 2D cannot accommodate the exponentially growing "volume" of moderate distances from high-D space.
\end{frame}

% Slide 4
\begin{frame}{The Paradigm Shift: From Geometry to Information}
\begin{center}
\begin{tikzpicture}[scale=1.3]
\node[draw, thick, fill=gray!20, minimum width=3cm, minimum height=1cm] (trad) at (0,3) {Traditional Methods};
\node[below of=trad, yshift=-0.5cm] {\textit{Preserve distances or variance}};
\draw[thick, red, ->] (0,1.5) -- (0,0.5);

\node[draw, thick, fill=highDcolor!20, minimum width=3cm, minimum height=1cm] (tsne) at (0,-1) {t-SNE};
\node[below of=tsne, yshift=-0.5cm] {\textit{Preserve probability distributions}};
\end{tikzpicture}
\end{center}

\vspace{0.3cm}
\begin{block}{Key Insight}
Instead of asking "How close are points?" we ask "How likely are they neighbors?"
\end{block}
\end{frame}


% Slide 5b - IMPROVED AESTHETIC VERSION
\begin{frame}{The Paradigm Shift: Concrete Example}
\begin{columns}
\column{0.5\textwidth}
\begin{tcolorbox}[colback=blue!5, colframe=blue!40, title={\textbf{Traditional: Preserve Distances}}]
\begin{center}
\begin{tikzpicture}[scale=0.8]
% Three points with better styling
\node[circle, draw=blue!70, fill=blue!20, line width=1.5pt, minimum size=10mm] (A) at (0,0) {\textbf{A}};
\node[circle, draw=red!70, fill=red!20, line width=1.5pt, minimum size=10mm] (B) at (2.5,0) {\textbf{B}};
\node[circle, draw=green!70, fill=green!20, line width=1.5pt, minimum size=10mm] (C) at (5,0) {\textbf{C}};

% Distance arcs
\draw[<->, thick, blue!60] ([yshift=-8mm]A.south) -- ([yshift=-8mm]B.south) node[midway, below] {$d=2$};
\draw[<->, thick, blue!60] ([yshift=-14mm]A.south) -- ([yshift=-14mm]C.south) node[midway, below] {$d=4$};
\end{tikzpicture}
\end{center}
\vspace{3mm}
\textcolor{red!70}{Problem:} All distances treated equally\\
\textcolor{gray}{No context about local density}
\end{tcolorbox}

\column{0.5\textwidth}
\begin{tcolorbox}[colback=green!5, colframe=green!40, title={\textbf{t-SNE: Preserve Probabilities}}]
\begin{center}
\begin{tikzpicture}[scale=0.8]
% Three points with probability visualization
\node[circle, draw=blue!70, fill=blue!20, line width=1.5pt, minimum size=10mm] (A) at (0,0) {\textbf{A}};
\node[circle, draw=red!70, fill=red!20, line width=1.5pt, minimum size=10mm] (B) at (2.5,0) {\textbf{B}};
\node[circle, draw=green!70, fill=green!20, line width=1.5pt, minimum size=10mm] (C) at (5,0) {\textbf{C}};

% Probability arrows with varying thickness
\draw[->, line width=3pt, red!60, opacity=0.7] (A) -- (B) node[midway, above] {\small $p=0.7$};
\draw[->, line width=1pt, green!60, opacity=0.7] (A) -- (C) node[midway, above] {\small $p=0.3$};
\end{tikzpicture}
\end{center}
\vspace{3mm}
\textcolor{green!70}{Solution:} Likelihood encodes context\\
\textcolor{gray}{Adapts to local density automatically}
\end{tcolorbox}
\end{columns}

\vspace{0.5cm}
\begin{tcolorbox}[colback=yellow!10, colframe=orange!60, boxrule=1pt]
\centering
\textbf{Key Insight:} Same distance $\rightarrow$ different probabilities based on neighborhood density
\end{tcolorbox}
\end{frame}


\begin{frame}{Building Intuition: From Distances to Neighborhoods}
\begin{columns}
\column{0.5\textwidth}
\textbf{The Problem with Raw Distances:}
\begin{itemize}
\item Point A: 1 unit from B, 10 units from C
\item But what if A is in dense region?
\item And C is in sparse region?
\item Raw distance loses context!
\end{itemize}

\vspace{0.3cm}
\textbf{The Solution - Relative Similarity:}
\begin{itemize}
\item Convert distances to probabilities
\item "How likely is B to be A's neighbor?"
\item Adapt to local density automatically
\item Use Gaussian decay (smooth, differentiable)
\end{itemize}

\column{0.5\textwidth}
\begin{center}
\begin{tikzpicture}[scale=0.9]
% Dense region example
\node at (0,2) {\small\textbf{Dense Region}};
\fill[blue!20] (-1,0) circle (1.2);
\fill[red] (-1,0) circle (3pt);
\foreach \angle in {0,45,...,315} {
    \fill[blue] (-1,0) ++(\angle:0.3) circle (2pt);
}
\node at (-1,-1.5) {\small Small $\sigma$ needed};

% Sparse region example  
\node at (3,2) {\small\textbf{Sparse Region}};
\fill[blue!10] (3,0) circle (1.2);
\fill[red] (3,0) circle (3pt);
\foreach \angle in {0,90,180,270} {
    \fill[blue] (3,0) ++(\angle:0.8) circle (2pt);
}
\node at (3,-1.5) {\small Large $\sigma$ needed};
\end{tikzpicture}
\end{center}

\vspace{0.2cm}
\textbf{Key Idea:} Each point gets its own "neighborhood size" ($\sigma_i$) based on local density
\end{columns}
\end{frame}

% Slide 5
\begin{frame}{From Distances to Probabilities}
\begin{columns}
\column{0.5\textwidth}
\begin{center}
\begin{tikzpicture}[scale=0.8]
\begin{axis}[
    xlabel={Distance},
    ylabel={Probability},
    width=7cm,
    height=5cm,
    grid=major,
    legend pos=north east
]
\addplot[domain=0:4, samples=100, thick, highDcolor] {exp(-x^2/2)};
\addlegendentry{Gaussian}
\addplot[domain=0:4, samples=100, thick, orange, dashed] {exp(-x^2/4)};
\addlegendentry{Wide $\sigma$}
\addplot[domain=0:4, samples=100, thick, red, dotted] {exp(-x^2/1)};
\addlegendentry{Narrow $\sigma$}
\end{axis}
\end{tikzpicture}
\end{center}

\column{0.5\textwidth}
\textbf{Key Transformation:}
$$p_{j|i} = \frac{\exp(-\|x_i - x_j\|^2/2\sigma_i^2)}{\sum_{k \neq i} \exp(-\|x_i - x_k\|^2/2\sigma_i^2)}$$

\vspace{0.3cm}
\insight{$\sigma_i$ adapts to local density automatically}
\end{columns}
\end{frame}


% Slide 6a - INTUITIVE VERSION
\begin{frame}{Why Gaussian? The Natural Choice for Neighborhoods}
\begin{columns}
\column{0.5\textwidth}
\begin{tcolorbox}[colback=blue!5, colframe=blue!40, title={\textbf{What We're Building}}]
\textbf{Our Goal:}
\begin{itemize}
\item Convert distances to probabilities
\item "How likely is j to be i's neighbor?"
\item Must adapt to local density
\end{itemize}

\vspace{3mm}
\textbf{Three Key Requirements:}
\begin{enumerate}
\item \textcolor{blue}{Smooth decay} - no sudden cutoffs
\item \textcolor{green}{Local focus} - neighbors matter most
\item \textcolor{red}{Unbiased} - don't assume patterns
\end{enumerate}

\vspace{3mm}
\textbf{The Winner: Gaussian}
\vspace{-2mm}
$p_{j|i} \propto e^{-d_{ij}^2/2\sigma_i^2}$
\end{tcolorbox}

\column{0.5\textwidth}
\begin{center}
\textbf{Visual Comparison}\\[3mm]
\begin{tikzpicture}[scale=0.9]
\begin{axis}[
    width=7cm,
    height=5cm,
    xlabel={Distance},
    ylabel={Probability},
    legend pos=north east,
    grid=major,
    ymin=0, ymax=1
]
% Step function - too rigid
\addplot[domain=0:3, samples=50, thick, red, dashed] {0.9*(x<1)};
\addlegendentry{Step (too rigid)}

% Linear - unnatural
\addplot[domain=0:3, samples=50, thick, orange] {max(0, 1-x/3)};
\addlegendentry{Linear (unnatural)}

% Gaussian - just right
\addplot[domain=0:3, samples=100, thick, blue, line width=2pt] {exp(-x^2/2)};
\addlegendentry{Gaussian (just right)}
\end{axis}
\end{tikzpicture}
\end{center}

\vspace{3mm}
\begin{tcolorbox}[colback=green!5, colframe=green!40, boxrule=0.5pt]
\centering\small
\textbf{Analogy:} Friendship strength\\
strongest nearby, fading smoothly
\end{tcolorbox}
\end{columns}
\end{frame}

% Slide 6b - MATHEMATICAL PRINCIPLE
\begin{frame}{The Mathematics Behind Gaussian: Maximum Entropy Principle}
\begin{columns}
\column{0.55\textwidth}
\begin{tcolorbox}[colback=yellow!5, colframe=orange!40, title={\textbf{The Core Principle}}]
\textbf{The Question:}\\
Which distribution makes the \textit{fewest assumptions} while matching our constraints?

\vspace{4mm}
\textbf{Answer: Maximum Entropy}\\
The distribution with highest uncertainty (entropy) given the constraints

\vspace{4mm}
\textbf{Why This Matters:}
\begin{itemize}
\item Most "honest" - no hidden bias
\item Adds no assumptions
\item Principled approach
\end{itemize}
\end{tcolorbox}

\column{0.45\textwidth}
\begin{center}
\textbf{Entropy Comparison}\\[3mm]
\begin{tikzpicture}[scale=0.8]
\draw[->] (0,0) -- (4.5,0) node[right] {\small Distance};
\draw[->] (0,0) -- (0,3) node[above] {\small Entropy};

% Bars showing entropy levels
\fill[red!40] (0.5,0) rectangle (1.2,0.8);
\node at (0.85,1.1) {\tiny Step};
\node at (0.85,0.4) {\tiny Low};

\fill[orange!40] (1.7,0) rectangle (2.4,1.5);
\node at (2.05,1.8) {\tiny Linear};
\node at (2.05,0.75) {\tiny Med};

\fill[blue!40] (2.9,0) rectangle (3.6,2.5);
\node at (3.25,2.8) {\tiny \textbf{Gaussian}};
\node at (3.25,1.25) {\tiny \textbf{Max}};

\draw[dashed, gray] (0,2.5) -- (4.5,2.5);
\end{tikzpicture}
\end{center}

\vspace{3mm}
\begin{tcolorbox}[colback=blue!5, colframe=blue!40]
\centering\small
\textbf{Gaussian = Maximum Entropy}\\
Most uncertain = Least biased
\end{tcolorbox}
\end{columns}
\end{frame}

% --- SLIDE 1: The Optimization Problem ---
\begin{frame}{The Mathematical Derivation: Problem Setup}
    \begin{tcolorbox}[colback=yellow!5, colframe=orange!40, title={\textbf{Optimization Problem}}]
    \textbf{Maximize Entropy:}
    \[H(P_i) = -\sum_j p_{j|i} \log p_{j|i}\]

    \textbf{Subject to Constraints:}
    \begin{enumerate}
    \item Normalization: $\sum_j p_{j|i} = 1$
    \item Fixed Variance: $\sum_j p_{j|i} d_{ij}^2 = \sigma_i^2$
    \end{enumerate}
    
    \vspace{2mm}
    \textit{The goal is to find the most unbiased probability distribution ($p_{j|i}$) that meets our constraints.}
    \end{tcolorbox}
\end{frame}


% --- SLIDE 2: The Solution ---
\begin{frame}{The Mathematical Derivation: Solution}
    \begin{tcolorbox}[colback=blue!5, colframe=blue!40, title={\textbf{Solution via Lagrange Multipliers}}]
    
    \textbf{1. The Lagrangian:}
    \begin{align*}
    \mathcal{L} = H(P_i) &+ \lambda \left(\sum p_{j|i} - 1\right) \\
                         &+ \mu \left(\sum p_{j|i}d_{ij}^2 - \sigma_i^2\right)
    \end{align*}
    
    \vspace{1mm}
    \textbf{2. Taking derivatives and solving for $\frac{\partial \mathcal{L}}{\partial p_{j|i}} = 0$ yields the result.}
    \vspace{1mm}
    
    \textbf{3. Result (The Gaussian Distribution):}
    \[
    p_{j|i} = \frac{e^{-\frac{d_{ij}^2}{2\sigma_i^2}}}{\sum_k e^{-\frac{d_{ik}^2}{2\sigma_i^2}}}
    \]
    
    \end{tcolorbox}
\end{frame}


% Slide 7 - REVISED VERSION
\begin{frame}{Perplexity: Setting the Neighborhood Size}
\begin{columns}
\column{0.5\textwidth}
\begin{tcolorbox}[colback=blue!5, colframe=blue!40, title={\textbf{The Problem We're Solving}}]
\textbf{Question:} How many neighbors should each point consider?

\vspace{3mm}
\textbf{Challenge:} Different regions have different densities!
\begin{itemize}
\item Dense areas: Small $\sigma$ needed
\item Sparse areas: Large $\sigma$ needed
\end{itemize}

\vspace{3mm}
\textbf{Solution:} Perplexity - a user parameter that sets "effective" number of neighbors
\end{tcolorbox}

\column{0.5\textwidth}
\begin{center}
\textbf{Adaptive Neighborhoods}\\[3mm]
\begin{tikzpicture}[scale=0.8]
% Dense region
\fill[blue!20] (-1,1) circle (0.8);
\fill[red] (-1,1) circle (3pt);
\foreach \angle in {0,45,...,315} {
    \fill[blue] (-1,1) ++(\angle:0.25) circle (1.5pt);
}
\node at (-1,0) {\small Dense: $\sigma = 0.1$};
\draw[<->, thick] (-1.3,1) -- (-0.7,1);
\node at (-1,1.5) {\tiny 0.6};

% Sparse region  
\fill[blue!10] (2,1) circle (0.8);
\fill[red] (2,1) circle (3pt);
\foreach \angle in {0,90,180,270} {
    \fill[blue] (2,1) ++(\angle:0.6) circle (1.5pt);
}
\node at (2,0) {\small Sparse: $\sigma = 0.5$};
\draw[<->, thick] (1.4,1) -- (2.6,1);
\node at (2,1.5) {\tiny 1.2};
\end{tikzpicture}

\vspace{3mm}
\begin{tcolorbox}[colback=green!5, colframe=green!40]
\centering\small
Both have same perplexity = 5\\
Different $\sigma$ values!
\end{tcolorbox}
\end{center}
\end{columns}

\vspace{1mm}
\begin{tcolorbox}[colback=yellow!10, colframe=orange!60]
\centering
\textbf{Key Insight:} Perplexity is constant across all points, but $\sigma_i$ adapts to achieve it
\end{tcolorbox}
\end{frame}

% Slide 7b - MATHEMATICAL DEFINITION AND BINARY SEARCH
\begin{frame}{Perplexity: The Mathematics and Algorithm}
\begin{columns}
\column{0.5\textwidth}
\begin{tcolorbox}[colback=blue!5, colframe=blue!40, title={\textbf{Mathematical Definition}}]
\textbf{From Shannon Entropy:}
$$H(P_i) = -\sum_j p_{j|i} \log_2 p_{j|i}$$

\vspace{2mm}
\textbf{Perplexity:}
$$\text{Perp}(P_i) = 2^{H(P_i)}$$

\vspace{1mm}
\textbf{Interpretation:}
\begin{itemize}
\item Perp = 5 → "acts like" 5 neighbors
\item Perp = 30 → "acts like" 30 neighbors
\item Typical range: 5-50
\end{itemize}

\small\textit{Note: It's the exponential of entropy!}
\end{tcolorbox}

\column{0.5\textwidth}
\begin{tcolorbox}[colback=orange!5, colframe=orange!40, title={\textbf{Finding $\sigma_i$: Binary Search}}]
\small
\textbf{Why Binary Search?}\\
Perplexity increases with $\sigma$ monotonically

\vspace{2mm}
\begin{center}
\begin{tikzpicture}[scale=0.6]
\begin{axis}[
    width=6cm,
    height=4cm,
    xlabel={$\sigma$},
    ylabel={Perplexity},
    grid=major
]
\addplot[domain=0.1:2, samples=50, thick, blue] {5*ln(x+1)+5};
\addplot[dashed, red] coordinates {(0,30) (2,30)};
\node at (axis cs:1.2,32) {\tiny Target};
\end{axis}
\end{tikzpicture}
\end{center}

\textbf{Algorithm:}
\begin{enumerate}
\item Start with $\sigma = 1$
\item Compute current perplexity
\item Too high? → Decrease $\sigma$
\item Too low? → Increase $\sigma$
\item Repeat until converged
\end{enumerate}
\normalsize
\end{tcolorbox}
\end{columns}
\end{frame}


% Slide 8 - REVISED VERSION
\begin{frame}{Measuring Information Loss: KL Divergence}
\vspace{-3mm}
\begin{columns}
\column{0.55\textwidth}
\begin{block}{What is KL Divergence?}
$$\text{KL}(P||Q) = \sum_j p_j \log\frac{p_j}{q_j}$$
\small
\textit{Extra bits needed when using $Q$ instead of true $P$}
\end{block}

\vspace{2mm}
\begin{block}{Critical Asymmetry Example}
\small
Consider point B with true probability 0.3:\\[2mm]
\textcolor{red}{\textbf{Missing a true neighbor:}}\\
True: $p = 0.3$, Embedded: $q = 0.01$\\
Penalty: $0.3 \times \log(30) \approx \textbf{1.02 bits}$\\[2mm]

\textcolor{blue}{\textbf{Creating a false neighbor:}}\\
True: $p = 0.01$, Embedded: $q = 0.3$\\
Penalty: $0.01 \times \log(0.033) \approx \textbf{-0.035 bits}$
\end{block}

\column{0.45\textwidth}
\begin{center}
\textbf{Visual Example}\\[2mm]
\begin{tikzpicture}[scale=0.8]
\begin{axis}[
  ybar,
  width=6cm,
  height=5cm,
  ylabel={Probability},
  xlabel={Points},
  symbolic x coords={A,B,C,D},
  xtick=data,
  ymin=0, ymax=0.5,
  legend pos=north east,
  legend style={font=\tiny}
]
\addplot[fill=blue!60] coordinates {(A,0.4) (B,0.3) (C,0.2) (D,0.1)};
\addplot[fill=red!60] coordinates {(A,0.35) (B,0.01) (C,0.4) (D,0.24)};
\legend{$P$ (truth), $Q$ (embedding)}
\end{axis}

% Annotations showing penalties
\draw[->, thick, red] (1.3, 2.5) -- (1.3, 0.5);
\node[red] at (1.3, 3) {\tiny Lost!};
\node[red] at (1.3, 3.3) {\tiny 1.02 bits};

\draw[->, thick, blue] (2.5, 1) -- (2.5, 2.3);
\node[blue] at (2.5, 0.5) {\tiny False};
\node[blue] at (2.5, 0.2) {\tiny -0.035 bits};
\end{tikzpicture}
\end{center}
\end{columns}

\vspace{2mm}
\begin{center}
\colorbox{yellow!20}{\parbox{0.9\textwidth}{\centering
\textbf{Key Insight:} t-SNE heavily penalizes separating true neighbors (30× more than false neighbors!)}}
\end{center}
\end{frame}



% Slide 9a - CONCEPTUAL INTRODUCTION
\begin{frame}{Original SNE: The Precursor to t-SNE}
\begin{center}
\textbf{A Brief History of Dimension Reduction}
\end{center}

\vspace{5mm}
\begin{tikzpicture}[scale=1.2]
% Timeline
\draw[thick, ->, gray] (0,0) -- (10,0);

% PCA
\fill[blue] (1,0) circle (2pt);
\draw[blue] (1,0) -- (1,0.5);
\node[above, blue] at (1,0.5) {PCA};
\node[below] at (1,-0.2) {\tiny 1901};
\node[below] at (1,-0.5) {\tiny Linear};

% MDS
\fill[purple] (3,0) circle (2pt);
\draw[purple] (3,0) -- (3,0.5);
\node[above, purple] at (3,0.5) {MDS};
\node[below] at (3,-0.2) {\tiny 1950s};
\node[below] at (3,-0.5) {\tiny Distances};

% SNE
\fill[red] (5,0) circle (2pt);
\draw[red] (5,0) -- (5,0.5);
\node[above, red] at (5,0.5) {SNE};
\node[below] at (5,-0.2) {\tiny 2002};
\node[below] at (5,-0.5) {\tiny Probabilities};

% Problem
\draw[orange, thick] (6,0.3) -- (7,0.3);
\node[orange] at (6.5,0.6) {\small Crowding};
\node[orange] at (6.5,0.9) {\small Problem};

% t-SNE
\fill[green] (8,0) circle (2pt);
\draw[green] (8,0) -- (8,0.5);
\node[above, green] at (8,0.5) {t-SNE};
\node[below] at (8,-0.2) {\tiny 2008};
\node[below] at (8,-0.5) {\tiny Solution!};
\end{tikzpicture}

\vspace{8mm}
\begin{columns}
\column{0.5\textwidth}
\begin{block}{SNE's Innovation}
\begin{itemize}
\item First to use probabilities
\item Adaptive neighborhoods ($\sigma_i$)
\item Information-theoretic approach
\item KL divergence for optimization
\end{itemize}
\end{block}

\column{0.5\textwidth}
\begin{block}{SNE's Fatal Flaw}
\begin{itemize}
\item Used Gaussian in low-D space
\item Cannot represent moderate distances
\item Led to "crowding problem"
\item All points collapse together
\end{itemize}
\end{block}
\end{columns}

\vspace{1mm}
\begin{center}
\colorbox{yellow!20}{\parbox{0.9\textwidth}{\centering
\textbf{Key Lesson:} Great ideas can fail on one crucial detail}}
\end{center}
\end{frame}


\begin{frame}{SNE's Mathematics: Where It Went Wrong}
% --- 2. Use [T] to top-align the columns ---
\begin{columns}[T]

    % --- Left Column: The Formulation ---
    \column{0.5\textwidth}
\begin{block}{The Formulation}
    \begin{itemize}
        \item \textbf{High-D Similarity ($P$):} \\
              Gaussian with adaptive variance $\sigma_i$
              $$ p_{j|i} = \frac{\exp(-d_{ij}^2/2\sigma_i^2)}{\sum_k \exp(-d_{ik}^2/2\sigma_i^2)} $$

        \item \textbf{Low-D Similarity ($Q$):} \\
              \alert{Gaussian with fixed variance}
              $$ q_{j|i} = \frac{\exp(-d_{ij}^2)}{\sum_k \exp(-d_{ik}^2)} $$

        % CORRECTED LINE: The formula is now inline to save vertical space.
        \item \textbf{Cost Function:} $C = \sum_i \mathrm{KL}(P_i || Q_i)$
    \end{itemize}
\end{block}

% --- Right Column: The Problem in 2D ---
\column{0.5\textwidth}
\begin{block}{Why Gaussian Fails in 2D}
    \centering % Center the plot
    \begin{tikzpicture}
        \begin{axis}[
            width=\linewidth,
            height=4.5cm, % CORRECTED: Reduced height from 5cm
            xlabel={Distance},
            ylabel={Similarity},
            ymode=log,
            grid=major,
            legend pos=north east,
            label style={font=\small},
            tick label style={font=\small}
        ]
        \addplot[domain=0:4, samples=50, thick, red!80!black] {exp(-x^2)};
        \addlegendentry{Gaussian}
        
        \addplot[domain=0:4, samples=50, thick, blue!80!black, dashed] {1/(1+x^2)};
        \addlegendentry{Student-t}
        
        \draw[<->, orange, thick] (axis cs:1.5, 5e-3) -- (axis cs:3, 5e-3)
            node[midway, below, font=\small] {Problem Area};
        \end{axis}
    \end{tikzpicture}
    \small
    \alert{Problem:} Moderate distances in high-D get exponentially tiny similarities in low-D, causing crowding.
\end{block}

\end{columns}


\end{frame}

% Slide 10 - REVISED VERSION
\begin{frame}{The Curse: Why High-D Breaks Our Intuition}
\vspace{-3mm}
\begin{columns}
\column{0.5\textwidth}
\begin{block}{The Volume Problem}
\textbf{Question:} In a D-dimensional sphere,\\
what fraction of volume is in the outer\\
shell (radius 0.9 to 1.0)?

\vspace{3mm}
\textbf{Your intuition (2D):}
$$\frac{\pi \cdot 1^2 - \pi \cdot 0.9^2}{\pi \cdot 1^2} = 19\%$$

\textbf{Reality in high-D:}
\begin{itemize}
\item 5D: 41\%
\item 10D: 65\%
\item 50D: 99.5\%
\item \textcolor{red}{\textbf{100D: 99.997\%}}
\end{itemize}
\end{block}

\column{0.5\textwidth}
\begin{center}
\textbf{Volume Distribution by Dimension}\\[2mm]
\begin{tikzpicture}[scale=0.7]
\begin{axis}[
  xlabel={Dimension},
  ylabel={Shell Volume (\%)},
  width=7cm,
  height=5cm,
  grid=major,
  ymin=0, ymax=100,
  legend pos=south east
]
\addplot[mark=*, thick, red, mark size=2pt] coordinates {
  (2,19) (5,41) (10,65) (20,88) (50,99.5) (100,99.997)
};
\addlegendentry{Shell (0.9-1.0)}

% Highlight the key point
\draw[red, very thick] (axis cs:100,99.997) circle (0.3);
\node[red] at (axis cs:80,90) {\small Almost ALL};
\node[red] at (axis cs:80,85) {\small volume here!};
\draw[red, ->] (axis cs:75,87) -- (axis cs:95,99);
\end{axis}
\end{tikzpicture}
\end{center}
\end{columns}

\vspace{3mm}
\begin{center}
\colorbox{orange!20}{\parbox{0.9\textwidth}{\centering
\textbf{Connection to SNE:} In high-D, all points are at similar distances (outer shell)\\
In 2D, we need room for varying distances that doesn't geometrically exist!}}
\end{center}
\end{frame}

% Slide 11
\begin{frame}{SNE's Fatal Flaw Visualized}
\begin{columns}
\column{0.5\textwidth}
\begin{center}
\textbf{High-D: Room for all}\\[0.3cm]
\begin{tikzpicture}[scale=1.2]
\foreach \r/\c in {0.5/green, 0.8/yellow, 1.1/orange, 1.4/red} {
    \foreach \a in {0,60,...,300} {
        \fill[\c!70] (\a:\r) circle (2pt);
    }
}
\node at (0,-2) {Distinct distances};
\end{tikzpicture}
\end{center}

\column{0.5\textwidth}
\begin{center}
\textbf{2D with Gaussian: Crushed!}\\[0.3cm]
\begin{tikzpicture}[scale=1.2]
\shade[inner color=white, outer color=red!30] (0,0) circle (0.5);
\foreach \i in {1,...,20} {
    \pgfmathsetmacro\r{0.3*rnd}
    \pgfmathsetmacro\a{360*rnd}
    \fill[black!50] (\a:\r) circle (1.5pt);
}
\node at (0,-2) {Cannot represent};
\node at (0,-2.5) {moderate distances};
\end{tikzpicture}
\end{center}
\end{columns}

\vspace{0.3cm}
\begin{center}
\Large\textbf{Solution: Use distribution with heavier tails!}
\end{center}
\end{frame}


% Slide 12 - REVISED VERSION
\begin{frame}{The t-SNE Innovation: Student-t Distribution}
\vspace{-3mm}
\begin{columns}
\column{0.55\textwidth}
\begin{block}{The Key Change}
\textbf{SNE (Gaussian in 2D):}
$$q_{ij} = \frac{e^{-d_{ij}^2}}{\sum_{k \neq l} e^{-d_{kl}^2}}$$

\textbf{t-SNE (Student-t in 2D):}
$$q_{ij} = \frac{(1 + d_{ij}^2)^{-1}}{\sum_{k \neq l} (1 + d_{kl}^2)^{-1}}$$

\vspace{1mm}
\textbf{Mathematical Properties:}
\begin{itemize}
\item Polynomial decay: $O(d^{-2})$ vs exponential
\item Heavy tails preserve moderate distances
\item Cauchy distribution (df = 1)
\end{itemize}
\end{block}

\column{0.45\textwidth}
\begin{center}
\textbf{Decay Comparison}\\[2mm]
\begin{tikzpicture}[scale=0.7]
\begin{axis}[
    xlabel={Distance},
    ylabel={Similarity},
    width=6cm,
    height=4.5cm,
    grid=major,
    legend pos=north east,
    domain=0:4,
    ymode=log,
    ymin=0.001, ymax=1
]
\addplot[samples=100, thick, blue] {exp(-x^2)};
\addlegendentry{Gaussian}
\addplot[samples=100, thick, red, line width=1.5pt] {1/(1+x^2)};
\addlegendentry{Student-t}

% Mark the key difference region
\fill[green, opacity=0.2] (axis cs:1.5,0.001) rectangle (axis cs:3,0.1);
\node at (axis cs:2.25,0.02) {\tiny Room for};
\node at (axis cs:2.25,0.008) {\tiny moderate};
\node at (axis cs:2.25,0.003) {\tiny distances};
\end{axis}
\end{tikzpicture}
\end{center}
\end{columns}

\vspace{3mm}
\begin{center}
\colorbox{green!20}{\parbox{0.9\textwidth}{\centering
\textbf{How it solves crowding:} Heavy tails provide "space" for moderate distances\\
that doesn't exist geometrically in 2D but exists mathematically in the similarity function}}
\end{center}
\end{frame}

% Slide 13
\begin{frame}{Quantifying the Solution}
\begin{block}{Similarity Ratio Analysis}
For distances $d_1 = 1$ and $d_2 = 3$:
  \end{block}

\begin{columns}
\column{0.5\textwidth}
\textbf{Gaussian:}
$$\frac{q(d_1)}{q(d_2)} = \frac{e^{-1}}{e^{-9}} = e^8 \approx 2981$$
  \textcolor{red}{Moderate distance becomes "infinite"}

\column{0.5\textwidth}
\textbf{Student-t:}
$$\frac{q(d_1)}{q(d_2)} = \frac{1/(1+1)}{1/(1+9)} = 5$$
  \textcolor{green}{Moderate distance preserved}
\end{columns}

\vspace{0.5cm}
\begin{center}
\colorbox{yellow!30}{\Large 600× difference in representation capacity!}
\end{center}
\end{frame}


% Slide 14a - THE THREE KEY MODIFICATIONS
\begin{frame}{From SNE to t-SNE: Three Critical Changes}
\vspace{-3mm}
\begin{center}
\Large\textbf{The Evolution}
\end{center}

\vspace{1mm}
\begin{block}{Modification 1: Symmetrization}
\textbf{SNE:} Asymmetric $p_{j|i} \neq p_{i|j}$\\
\textbf{t-SNE:} Symmetric $p_{ij} = p_{ji} = \frac{p_{j|i} + p_{i|j}}{2n}$

\vspace{1mm}
\textbf{Why?} 
\begin{itemize}
\item Simplifies gradient (one term instead of two)
\item Ensures outliers get fair representation
\item More elegant optimization
\end{itemize}
\end{block}

\begin{block}{Modification 2: Student-t in Low-D}
\textbf{SNE:} $q_{ij} = \frac{\exp(-d_{ij}^2)}{\sum_{k \neq l}\exp(-d_{kl}^2)}$ (Gaussian)\\
\textbf{t-SNE:} $q_{ij} = \frac{(1+d_{ij}^2)^{-1}}{\sum_{k \neq l}(1+d_{kl}^2)^{-1}}$ (Student-t)

\vspace{1mm}
\textbf{Why?} Solves crowding with heavy tails
\end{block}

\begin{block}{Modification 3: Single KL Divergence}
\textbf{SNE:} $C = \sum_i \text{KL}(P_i||Q_i)$ (sum of individual KLs)\\
\textbf{t-SNE:} $C = \text{KL}(P||Q)$ (single joint KL)

\vspace{1mm}
\textbf{Why?} Works with symmetric probabilities
\end{block}
\end{frame}

% Slide 14b - THE UNIFIED ALGORITHM
\begin{frame}{The Complete t-SNE Algorithm}
\vspace{-3mm}
\begin{columns}
\column{0.5\textwidth}
\begin{block}{Input → Probabilities}
\textbf{1. Compute pairwise affinities:}
\small
$$p_{j|i} = \frac{\exp(-\|x_i-x_j\|^2/2\sigma_i^2)}{\sum_k \exp(-\|x_i-x_k\|^2/2\sigma_i^2)}$$
\normalsize

\textbf{2. Symmetrize:}
$$p_{ij} = \frac{p_{j|i} + p_{i|j}}{2n}$$

\textbf{3. Early exaggeration:}
$$p_{ij} \leftarrow 4 \cdot p_{ij} \text{ (first 250 iter)}$$
\end{block}

\column{0.5\textwidth}
\begin{block}{Optimization}
\textbf{4. Initialize:} $y_i \sim \mathcal{N}(0, 10^{-4})$

\textbf{5. Compute low-D similarities:}
\small
$$q_{ij} = \frac{(1+\|y_i-y_j\|^2)^{-1}}{\sum_{k \neq l}(1+\|y_k-y_l\|^2)^{-1}}$$
\normalsize

\textbf{6. Update via gradient:}
\small
$$\frac{\partial C}{\partial y_i} = 4\sum_j (p_{ij} - q_{ij})(y_i - y_j)(1 + d_{ij}^2)^{-1}$$
\normalsize

\textbf{7. Iterate until convergence}
\end{block}
\end{columns}

\vspace{3mm}
\begin{center}
\colorbox{blue!20}{\parbox{0.9\textwidth}{\centering
\textbf{Result:} An elegant algorithm that preserves local structure while solving crowding}}
\end{center}
\end{frame}

% Slide 15
\begin{frame}{Understanding the Gradient: Force Interpretation}
\begin{center}
\begin{tikzpicture}[scale=1.5]
\fill[red] (0,0) circle (3pt);
\node[above] at (0,0.2) {$y_i$};

\foreach \angle/\dist in {30/1.5, 90/1.2, 150/1.3} {
  \draw[->, thick, green!70, >=stealth] (0,0) -- (\angle:\dist);
  \fill[green] (\angle:\dist) circle (2pt);
}

\foreach \angle/\dist in {210/1, 270/0.8, 330/1.1} {
  \draw[<-, thick, red!70, >=stealth] (\angle:\dist) -- (\angle:0.3);
  \fill[gray] (\angle:\dist) circle (2pt);
}

\node at (0,-2) {$\nabla C = 4\sum_j \underbrace{(p_{ij} - q_{ij})}_{\text{error}} \underbrace{(y_i - y_j)}_{\text{direction}} \underbrace{(1 + d_{ij}^2)^{-1}}_{\text{adaptive weight}}$};
\end{tikzpicture}
\end{center}

\insight{Weight term prevents distant clusters from merging}
\end{frame}


% Slide 16a - EARLY EXAGGERATION
\begin{frame}{Optimization Trick 1: Early Exaggeration}
\begin{columns}
\column{0.5\textwidth}
\begin{block}{The Technique}
\textbf{What:} Multiply $P$ by 4 for first 250 iterations
$$p_{ij}^{\text{early}} = 4 \cdot p_{ij}$$

\textbf{Effect on forces:}
\begin{itemize}
\item True neighbors pull 4× harder
\item Clusters form quickly
\item Global structure emerges first
\end{itemize}
\end{block}

\column{0.5\textwidth}
\begin{center}
\textbf{Visual Effect}\\[3mm]
\begin{tikzpicture}[scale=0.7]
% Before
\draw[gray] (-1.5,-0.5) rectangle (1.5,0.5);
\foreach \i in {1,...,15} {
    \pgfmathsetmacro\x{3*rnd-1.5}
    \pgfmathsetmacro\y{rnd-0.5}
    \fill[blue!50] (\x,\y) circle (1pt);
}
\node at (0,-0.8) {\small Random start};
\end{tikzpicture}

\begin{tikzpicture}[scale=0.7]
% After
\draw[gray] (-1.5,-0.5) rectangle (1.5,0.5);
\foreach \i in {1,...,5} {
    \pgfmathsetmacro\r{0.15*rnd}
    \pgfmathsetmacro\a{72*rnd}
    \fill[red!70] (-0.8,0) ++(\a:\r) circle (1.5pt);
    \fill[green!70] (0.8,0) ++(\a:\r) circle (1.5pt);
}
\node at (0,-0.8) {\small After 250 iter};
\end{tikzpicture}
\end{center}
\end{columns}

\vspace{3mm}
\begin{center}
\colorbox{green!20}{\parbox{0.85\textwidth}{\centering
Strong initial forces prevent poor local arrangements}}
\end{center}
\end{frame}

% Slide 16b - MOMENTUM
\begin{frame}{Optimization Trick 2: Momentum}
\begin{columns}
\column{0.5\textwidth}
\begin{block}{The Technique}
\textbf{Update equation:}
\small
$$\Delta y_i^{(t)} = -\eta \nabla_i + \alpha(t) \Delta y_i^{(t-1)}$$
\normalsize

\textbf{Schedule:}
$$\alpha(t) = \begin{cases}
0.5 & t < 250 \\
0.8 & t \geq 250
\end{cases}$$

\textbf{Benefits:}
\begin{itemize}
\item Escapes local minima
\item Smooths optimization
\item Reduces oscillations
\end{itemize}
\end{block}

\column{0.5\textwidth}
\begin{center}
\textbf{Effect on Optimization}\\[3mm]
\begin{tikzpicture}[scale=0.7]
% Cost landscape
\draw[thick, gray!50] plot[smooth, tension=0.7] coordinates {(0,2) (1,1) (1.5,1.5) (2,0.5) (3,0.8) (4,0)};

% Without momentum
\draw[red, thick, ->] (0.5,1.8) -- (1,1.2);
\fill[red] (1,1.2) circle (2pt);
\node[red] at (1,1.5) {\tiny Stuck};

% With momentum
\draw[blue, thick, ->] (0.5,1.6) -- (2.5,0.4);
\draw[blue, thick, ->] (2.5,0.4) -- (4,0);
\fill[blue] (4,0) circle (2pt);
\node[blue] at (4,-0.3) {\tiny Success};
\end{tikzpicture}
\end{center}

\textbf{Analogy:} Ball rolling downhill - momentum carries it over bumps
\end{columns}
\end{frame}

% Slide 16c - ADAPTIVE LEARNING
\begin{frame}{Optimization Trick 3: Adaptive Learning Rate}
\begin{columns}
\column{0.5\textwidth}
\begin{block}{The Technique}
\textbf{Adaptation rule:}
\small
\begin{itemize}
\item Same direction: $\eta \times 1.2$
\item Direction change: $\eta \times 0.8$
\item Min: $\eta_{\min} = 0.01$
\item Max: $\eta_{\max} = 1000$
\end{itemize}
\normalsize

\textbf{Benefits:}
\begin{itemize}
\item Fast in flat regions
\item Careful near minima
\item Automatic adjustment
\end{itemize}
\end{block}

\column{0.5\textwidth}
\begin{center}
\textbf{Learning Rate Evolution}\\[3mm]
\begin{tikzpicture}[scale=0.6]
\begin{axis}[
    xlabel={\tiny Iteration},
    ylabel={\tiny Learning Rate},
    width=6cm,
    height=4cm,
    legend pos=north east,
    legend style={font=\tiny}
]
% Adaptive
\addplot[thick, red, mark=none] coordinates {
    (0,200) (100,180) (200,100) (300,80) (400,50)
};
\addlegendentry{Adaptive}

% Fixed
\addplot[thick, blue, dashed] coordinates {
    (0,200) (400,200)
};
\addlegendentry{Fixed}
\end{axis}
\end{tikzpicture}
\end{center}
\end{columns}

\vspace{3mm}
\begin{center}
\colorbox{green!20}{\parbox{0.85\textwidth}{\centering
\textbf{Combined:} 5× speedup (5000 → 1000 iterations)}}
\end{center}
\end{frame}


% Slide 17 - BARNES-HUT REVISED
\begin{frame}{Barnes-Hut: Scaling to Large Datasets}
\begin{columns}
\column{0.5\textwidth}
\begin{block}{The Algorithm}
\textbf{Key Idea:} Treat distant clusters as single points

\textbf{Steps:}
\begin{enumerate}
\item Build quadtree (2D) or octree (3D)
\item For each point, traverse tree
\item Apply criterion: $\frac{r_{\text{cell}}}{d_{\text{to cell}}} < \theta$
\item If satisfied, use center of mass
\end{enumerate}

\textbf{Parameter:} $\theta \in [0.5, 0.8]$\\
\small (higher = faster but less accurate)
\end{block}

\column{0.5\textwidth}
\begin{center}
\textbf{Tree Approximation}\\[2mm]
\begin{tikzpicture}[scale=0.8]
\draw[thick] (0,0) rectangle (4,4);
\draw (2,0) -- (2,4);
\draw (0,2) -- (4,2);
\draw (1,2) -- (1,4);
\draw (0,3) -- (2,3);

% Points
\foreach \i in {1,...,15} {
  \pgfmathsetmacro\x{4*rnd}
  \pgfmathsetmacro\y{4*rnd}
  \fill[blue] (\x,\y) circle (1pt);
}

% Highlight distant cell
\fill[red!30, opacity=0.5] (2,2) rectangle (4,4);
\node at (3,3) {\small 1 point};
\draw[<-] (3.5,3) -- (4.5,3.5) node[right] {\tiny Center of mass};
\end{tikzpicture}
\end{center}

\begin{center}
\begin{tabular}{l|r|r}
\textbf{Points} & \textbf{Exact} & \textbf{Barnes-Hut} \\
\hline
1,000 & 1 sec & 0.1 sec \\
10,000 & 100 sec & 2 sec \\
100,000 & 10,000 sec & 50 sec \\
\end{tabular}
\end{center}
\end{columns}
\end{frame}

% Slide 18 - DEBUGGING REVISED
\begin{frame}{Debugging t-SNE: Visual Diagnosis Guide}
\begin{center}
\textbf{Common Problems and Their Fixes}\\[3mm]
\begin{tikzpicture}[scale=0.85]
% Problem 1: Ball
\begin{scope}[shift={(0,0)}]
\shade[ball color=gray!30] (0,0) circle (0.6);
\foreach \i in {1,...,15} {
  \pgfmathsetmacro\r{0.2*rnd}
  \pgfmathsetmacro\a{24*\i}
  \fill[black!50] (\a:\r) circle (1pt);
}
\node at (0,-1) {\small\textbf{Ball}};
\node at (0,-1.3) {\tiny LR too low};
\node at (0,-1.6) {\tiny\textcolor{red}{Fix: LR > 100}};
\end{scope}

% Problem 2: Scattered
\begin{scope}[shift={(3,0)}]
\foreach \i in {1,...,15} {
  \pgfmathsetmacro\x{1.2*rnd-0.6}
  \pgfmathsetmacro\y{1.2*rnd-0.6}
  \fill[black!50] (\x,\y) circle (1pt);
}
\node at (0,-1) {\small\textbf{Scattered}};
\node at (0,-1.3) {\tiny LR too high};
\node at (0,-1.6) {\tiny\textcolor{red}{Fix: LR < 500}};
\end{scope}

% Problem 3: Fragmented
\begin{scope}[shift={(6,0)}]
\foreach \c in {0,120,240} {
  \foreach \i in {1,...,4} {
    \pgfmathsetmacro\r{0.15*rnd}
    \pgfmathsetmacro\a{\c+15*rnd}
    \fill[black!50] (\a:0.4+\r) circle (1pt);
  }
}
\node at (0,-1) {\small\textbf{Fragmented}};
\node at (0,-1.3) {\tiny Perp too low};
\node at (0,-1.6) {\tiny\textcolor{red}{Fix: Perp > 20}};
\end{scope}

% Problem 4: Good
\begin{scope}[shift={(9,0)}]
\foreach \c/\col in {45/blue,165/red,285/green} {
  \foreach \i in {1,...,6} {
    \pgfmathsetmacro\r{0.2*rnd}
    \pgfmathsetmacro\a{\c+20*rnd}
    \fill[\col!70] (\a:0.5+\r) circle (1.5pt);
  }
}
\node at (0,-1) {\small\textbf{Good!}};
\node at (0,-1.3) {\tiny Balanced};
\node at (0,-1.6) {\tiny\textcolor{green}{LR=200, Perp=30}};
\end{scope}
\end{tikzpicture}
\end{center}

\vspace{3mm}
\begin{center}
\colorbox{yellow!20}{\parbox{0.85\textwidth}{\centering
\textbf{Golden Rule:} Run multiple times with different seeds. Trust what's consistent.}}
\end{center}
\end{frame}

% Slide 19 - PERPLEXITY EFFECTS REVISED
\begin{frame}{Perplexity: Your Main Control Parameter}
\begin{columns}
\column{0.6\textwidth}
\begin{center}
\begin{tikzpicture}[scale=0.8]
% Perp = 5
\begin{scope}[shift={(0,0)}]
\foreach \c in {0,72,144,216,288} {
  \foreach \i in {1,...,3} {
    \pgfmathsetmacro\r{0.15*rnd}
    \pgfmathsetmacro\a{\c+15*rnd}
    \fill[blue!70] (\a:0.8+\r) circle (1.5pt);
  }
}
\node at (0,-1.3) {Perplexity = 5};
\node at (0,-1.6) {\tiny Many fragments};
\end{scope}

% Perp = 30
\begin{scope}[shift={(3.5,0)}]
\foreach \c/\col in {60/blue,180/red,300/green} {
  \foreach \i in {1,...,8} {
    \pgfmathsetmacro\r{0.25*rnd}
    \pgfmathsetmacro\a{\c+30*rnd}
    \fill[\col!70] (\a:0.6+\r) circle (1.5pt);
  }
}
\node at (0,-1.3) {Perplexity = 30};
\node at (0,-1.6) {\tiny Clear clusters};
\end{scope}

% Perp = 100
\begin{scope}[shift={(7,0)}]
\foreach \i in {1,...,25} {
  \pgfmathsetmacro\r{0.8*rnd}
  \pgfmathsetmacro\a{360*rnd}
  \fill[gray!70] (\a:\r) circle (1.5pt);
}
\node at (0,-1.3) {Perplexity = 100};
\node at (0,-1.6) {\tiny One blob};
\end{scope}
\end{tikzpicture}
\end{center}

\column{0.4\textwidth}
\begin{block}{How to Choose?}
\textbf{Rule of thumb:}\\
Perp = $\sqrt{N}/10$ to $\sqrt{N}/2$\\
\small (N = number of points)

\vspace{3mm}
\textbf{Examples:}
\begin{itemize}
\item 1,000 points: 5-15
\item 10,000 points: 20-50
\item 100,000 points: 50-150
\end{itemize}

\vspace{3mm}
\textbf{Strategy:}
\begin{enumerate}
\item Try 3 values (low, mid, high)
\item Look for consistency
\item Trust stable structures
\end{enumerate}
\end{block}
\end{columns}

\vspace{3mm}
\begin{center}
\colorbox{blue!20}{\parbox{0.85\textwidth}{\centering
\textbf{Key Insight:} Truth is what's consistent across multiple perplexity values}}
\end{center}
\end{frame}

% Slide 20 - CRITICAL WARNINGS (MINIMAL CHANGES)
\begin{frame}{Critical: What You CANNOT Interpret}
\begin{center}
\Large\textcolor{red}{\textbf{The Three Deadly Sins of t-SNE}}
\end{center}

\vspace{3mm}
\begin{columns}
\column{0.33\textwidth}
\begin{center}
\begin{tikzpicture}[scale=0.8]
\shade[ball color=blue!30] (-0.5,0) circle (0.8);
\shade[ball color=red!30] (0.5,0) circle (0.3);
\draw[red, thick] (0,-0.7) -- (0,0.7);
\draw[red, thick] (-0.7,0) -- (0.7,0);
\node at (0,-1.3) {\textbf{Sin \#1}};
\node at (0,-1.6) {\small Size $\neq$ Count};
\node at (0,-1.9) {\tiny\textcolor{red}{Visual area meaningless}};
\end{tikzpicture}
\end{center}

\column{0.33\textwidth}
\begin{center}
\begin{tikzpicture}[scale=0.8]
\fill[blue!50] (-1,0) circle (0.3);
\fill[red!50] (1,0) circle (0.3);
\draw[<->, thick] (-0.6,0) -- (0.6,0);
\node at (0,0.3) {\textcolor{red}{?}};
\draw[red, thick] (0,-0.5) -- (0,0.5);
\node at (0,-1.3) {\textbf{Sin \#2}};
\node at (0,-1.6) {\small Distance meaningless};
\node at (0,-1.9) {\tiny\textcolor{red}{Between clusters}};
\end{tikzpicture}
\end{center}

\column{0.33\textwidth}
\begin{center}
\begin{tikzpicture}[scale=0.8]
\fill[green!50] (0,0.5) circle (0.3);
\fill[orange!50] (0,-0.5) circle (0.3);
\draw[<->, red, thick] (-0.5,0.5) -- (-0.5,-0.5);
\node[red] at (-0.8,0) {\tiny ?};
\node at (0,-1.3) {\textbf{Sin \#3}};
\node at (0,-1.6) {\small Position arbitrary};
\node at (0,-1.9) {\tiny\textcolor{red}{No axes meaning}};
\end{tikzpicture}
\end{center}
\end{columns}

\vspace{5mm}
\begin{center}
\colorbox{yellow!20}{\parbox{0.85\textwidth}{\centering
\textbf{Remember:} Only local neighborhoods are meaningful. Everything else is artifact.}}
\end{center}
\end{frame}


\end{document}




\end{document}